[
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "CSCI 1051: Syllabus",
    "section": "",
    "text": "Course Description: Deep learning has revolutionized our ability to solve complex problems, but its success often obscures the underlying principles that make it work. This course offers a structured approach to understanding and applying deep learning techniques, beginning with the theoretical foundations of linear and logistic regression. We will then generalize these ideas into a three-step deep learning framework (architecture, loss function, and optimizer). Applying this framework, we will explore modern generative methods like auto-regressive language models and diffusion. Because the tools we discuss are so powerful, it is important to understand their limitations. To this end, we will also cover techniques for interpreting deep learning predictions and adding safeguards to generated content.\nPrerequisites: I will assume familiarity with calculus, linear regression, probability, and Python. In particular, I will assume you are comfortable with derivatives, the chain rule, gradients, matrix multiplication, and probability distributions.\nStructure: We will meet on Monday, Tuesday, Wednesday, and Thursday at 75 Shannon in Room 202. The lecture is from 10am to 12pm and the discussion is from 2 to 4pm. I will hold my office hours during the last hour of the discussion. If you would like to meet outside of these times, please email me.\nResources: This class is loosely based on Chinmay Hegde’s phenomenal graduate Deep Learning course at NYU Tandon. For each class, I will post my handwritten slides. I would also like for you to be able to access curated notes. To this end, one of the assignments in the class is to scribe notes on a lecture of your choice.\nDiscussion: Please post all your course related questions on Canvas. If your question reveals your solution to a homework problem, please email me instead.\n\nGrading\nYour grade in the class will be based on the number of points you earn. You will receive an A if you earn 93 or more points, an A- if you earn between 90 and 92 points (inclusive), a B+ if you earn between 87 and 89 points (inclusive), etc.\nParticipation and Questions (12 points): Since winter term classes are smaller, let’s take advantage of the opportunity for more engagement. Unless you have a reasonable excuse (e.g. sickness, family emergency), I expect you to attend every lecture and discussion. Whether you are able to attend or not, I expect you to fill out the form linked from the home page to receive credit for engagement (one point per lecture day that you fill it out). Of course, if you are not able to attend in person, you should watch the recorded zoom lecture before filling out the form.\nProblem Sets (48 points): There will be one problem per class. You may work on the problems with your classmates. However, you should write your solutions and code by yourself.\nProject (35 points): In order to practice implementing the ideas we cover, you will select a topic we cover in class and implement an algorithm we discussed on a data set of your choosing. You will write a report describing your results and what you learned. You will also give a presentation showcasing your code to the class. You can complete your project as an individual or with a partner.\nScribed Notes (9 points): Notes are a great resource for augmenting your in-class learning. While I have notes for the prior iteration of this course, I since updated the topics in this course to account for much of the progress in deep learning in the last two years. Unfortunately, I haven’t had time to create a new set of notes to reflect the current content. As such, I would like to ask for your help in writing these notes. I will provide an example for the first class (including the finished product and the source). Thereafter, students can sign up on this sheet to write notes for a lecture of your choice. You will have access to my handwritten slides, a zoom recording of the lecture, and any generative model. Because I hope to use these notes in future iterations of the course (with appropriate acknowledgements, of course), I will grade the notes stringently and allow you to resubmit them once.\nLate Policy: I expect all assignments to be turned in on time. If you are unable to turn in an assignment on time, you must email me before the assignment is due to request an extension.\n\n\nHonor Code\nAcademic integrity is an important part of your learning experience. You are welcome to use online material and discuss problems with others but you must explicitly acknowledge the outside resources on the work you submit.\nIf I notice that you have copied someone else’s work without proper attribution (such as code from the internet without a reference link or a solution very close to another student’s without giving credit), I will give you a warning. After the warning, I will subtract 5 points for every violation.\nLarge Language Models: LLMs (that we learn about in class!) are a powerful tool. However, while they are very good at producing human-like text, they have no inherent sense of ‘correctness’. You may use LLMs (as detailed below) but you are wholly responsible for the material you submit.\nYou may use LLMs for:\n\nImplementing short blocks of code that you can easily check.\nSimple questions whose answers you can easily verify.\n\nDo not use LLMS for:\n\nImplementing extensive code or code that you don’t understand.\nComplicated questions (like those on the problem sets) that you would learn from answering yourself.\n\nUltimately, the point of the assignments in this class are for you to practice the concepts. If you use an LLM in lieu of practice, then you deny yourself the chance to learn.\n\n\nAcademic Accommodations\nIf you have a Letter of Accommodation, please contact me as early in the term as possible. If you do not have a Letter of Accommodation and you believe you are eligible, please reach out to the ADA Coordinators at ada@middlebury.edu."
  },
  {
    "objectID": "notes/02_LogisticRegAndEntropy.html",
    "href": "notes/02_LogisticRegAndEntropy.html",
    "title": "Logistic Regression and Cross Entropy Loss",
    "section": "",
    "text": "Written by Julian Grijalva"
  },
  {
    "objectID": "notes/02_LogisticRegAndEntropy.html#motivation",
    "href": "notes/02_LogisticRegAndEntropy.html#motivation",
    "title": "Logistic Regression and Cross Entropy Loss",
    "section": "Motivation",
    "text": "Motivation\nIn many supervised learning settings, Linear regression is not appropriate because:\nThe output space is discrete rather than continuous (e.g., classifying emails as spam/not spam) We need probabilities to quantify uncertainty in our predictions. Linear regression can produce values outside [0,1], making probability interpretation impossible.\nClassification addresses these limitations by explicitly modeling the probability of class membership. Rather than outputting arbitrary real numbers, we want our models to output well-calibrated probabilities that can inform decision-making and risk assessment."
  },
  {
    "objectID": "notes/02_LogisticRegAndEntropy.html#supervised-binary-classification",
    "href": "notes/02_LogisticRegAndEntropy.html#supervised-binary-classification",
    "title": "Logistic Regression and Cross Entropy Loss",
    "section": "Supervised Binary Classification",
    "text": "Supervised Binary Classification\nAt times, we will build models with data whose output values are discrete rather than Linear. For these cases, it is not appropriate to use Linear Regression. Instead, these models will use an approach called Supervised Binary Classification. For these such models, imagine a dataset represented as:\n\\(\\mathcal{D} = \\{(x^i, y^i)\\}_{i=1}^n, \\; x^i \\in \\mathbb{R}^d, \\; y^i \\in \\{0, 1\\}\\)\nOur output value y, is represented as a binary value, either 1 for ‘True’ or 0 for ‘False’.\nOur end goal for this method is for our Model’s predicted output \\(F(x) = \\text{Output}\\) to equal the predicted probability of a possitive class (binary value 1). This allows us to later optimize the model by using optimizers which reward correct and confident results, while indicating a high level of loss for confident and incorrect results.\n\nSigmoid Function\nIn the context of logistic regression, the sigmoid function is a mathematical function used to map real-valued inputs into a probability range between 0 and 1. It is defined as:\n\\(\\sigma(z) = \\frac{1}{1 + e^{-z}}\\)\nThe sigmoid function ensures the output of logistic regression is bounded between 0 and 1, making it suitable for probabilistic interpretation.\n\n\n\nLogistic Sigmoid Graph\n\n\nCompared to Linear Regression, Logistic Regression Models use this sigmoid as an activation function to ensure output of a probability in the range [0,1].\n\n\n\nActivation application"
  },
  {
    "objectID": "notes/02_LogisticRegAndEntropy.html#general-classification-model",
    "href": "notes/02_LogisticRegAndEntropy.html#general-classification-model",
    "title": "Logistic Regression and Cross Entropy Loss",
    "section": "General Classification Model",
    "text": "General Classification Model\nWhen dealing with k &gt; 2 classes, we need to output a probability distribution over all possible classes. This requires:\nMultiple neurons: Each class gets its own weight vector \\(w_i\\) to compute class-specific scores Softmax activation: Converts raw scores into proper probabilities that sum to 1.\nFor input x, the model computes: \\(z_i = w_i^T x + b_i\\) for i = 1,…,k \\(f_i(x) = \\frac{e^{z_i}}{\\sum_{j=1}^k e^{z_j}}\\) The softmax function ensures:\nAll outputs are positive (due to exponential) Outputs sum to 1 (due to normalization) Preserves ordering of inputs (monotonic)\nApplication of softmax equation:\n\n\nCross Entropy Loss\nCross entropy provides a natural way to compare the predicted probability distribution with the true distribution encoded as one-hot vectors. For a single example with true class j: \\(H(y, f(x)) = -\\sum_{i=1}^k y_i \\log(f_i(x)) = -\\log(f_j(x))\\) This loss function has important properties:\nSimplifies to negative log probability of true class (since y is one-hot) Goes to infinity as predicted probability approaches 0, strongly penalizing confident wrong predictions Matches information theoretic notion of coding length Proper scoring rule (minimized by true probabilities).\n\n\n\nNegative Log graph\n\n\nThe negative log function’s shape explains these properties:\nSteep slope near 0 → large gradients for wrong predictions.\nShallow slope near 1 → small gradients for correct predictions.\nAsymptote at 0 → infinite loss for completely wrong predictions.\n\n\nbinary encoding and entropy\nIn these logistic deep learning models, discrete information must be given to the model by encoding categories using bit integer values. For example, assigning A,B,C,D to 00,01,10,11 is an example of fixed-length encoding. This is straightforward and works well when all labels are equally likely to occur. However, if the symbols are not equally likely, this approach is inefficient because it doesn’t take advantage of the differing probabilities.\nWhen the probability distribution of symbols is known, variable-length encoding becomes more efficient. The idea is to assign:\n-Shorter codes to more common symbols. -Longer codes to less common symbols.\nThis minimizes the expected number of bits required to encode a sequence of symbols, leading to more efficient storage or communication.\nBelow are examples of fixed length encoding vs variable-length encoding for a distribution with many C’s, some A’s, and less B and D’s:\n \nShannon’s entropy formula quantifies the minimum average number of bits required to encode symbols from a probability distribution:\n\\(H(q) = -\\sum_{i} p_i \\log_2(p_i)\\)\nIn order to find the entropy loss between two different distributions p and q:\n\\(H(p,q) = -\\sum_{i} p_i \\log_2(q_i)\\)"
  },
  {
    "objectID": "notes/02_LogisticRegAndEntropy.html#conclusion",
    "href": "notes/02_LogisticRegAndEntropy.html#conclusion",
    "title": "Logistic Regression and Cross Entropy Loss",
    "section": "Conclusion",
    "text": "Conclusion\nWe now have model of outputting predictions for classification and a loss. Logistic regression represents a fundamental approach to binary classification problems, distinct from linear regression by its ability to handle discrete outputs through probability estimation.\nExact computation won’t work like it did for linear regression, so we need a new strategy for this kind of problem.\nThe sigmoid function serves as the crucial bridge, transforming linear combinations of inputs into probabilities between 0 and 1, making it ideal for binary classification tasks.\nCross entropy loss plays a vital role in measuring model performance, providing a mathematically sound way to quantify the difference between predicted probabilities and actual binary labels. Its properties make it particularly effective for gradient descent optimization, as it heavily penalizes confident but incorrect predictions while rewarding accurate ones.\nThe connection to information theory through binary encoding and entropy demonstrates the broader theoretical foundation of these concepts. Whether using fixed-length or variable-length encoding, the choice of representation impacts the efficiency of information storage and processing. Shannon’s entropy formulas provide a mathematical framework for quantifying these efficiencies and measuring the divergence between probability distributions."
  },
  {
    "objectID": "notes/08_watermarking.html",
    "href": "notes/08_watermarking.html",
    "title": "Watermarking LLMs",
    "section": "",
    "text": "Written by Lenox Herman"
  },
  {
    "objectID": "notes/08_watermarking.html#motivation",
    "href": "notes/08_watermarking.html#motivation",
    "title": "Watermarking LLMs",
    "section": "Motivation",
    "text": "Motivation\nAs LLMs (large language models) continue to be used and grow in modern times, the need for delineating where text originates becomes more apparent. For instance, a teacher might want to determine if a student has used an LLM to complete an assignment. On the flip side, organizations like OpenAI might need a way to verify whether a given output can genuinely be attributed to their models. In order to do this, LLMs use watermarking to determine whether a block of text was human or machine-generated.\nLet’s look at the example, “Turmeric lemon cookies are…” In order to calculate the next word, LLMs embed each word to create a probability distribution to determine the next word."
  },
  {
    "objectID": "notes/08_watermarking.html#redgreen-listing",
    "href": "notes/08_watermarking.html#redgreen-listing",
    "title": "Watermarking LLMs",
    "section": "Red/Green Listing",
    "text": "Red/Green Listing\nThis is a watermarking method where words are predetermined as “Red” and “Green” words. Here, a small constant is added to all green words before the softmax is used to calculate the probability distribution. The result is that green words are slightly more likely to be selected when generating text.\nTo detect Red/Green watermarking on LLM-generated text, you would run a P-test. Consider the example:\n“Watermarking is a very fun topic to cover especially when Teal is teaching it.”\nHere we assume that the probability of green words appearing in natural text is 50/50, or 1/2.\nTo detect Red/Green watermarking in LLM-generated text, a statistical hypothesis test can be applied. Suppose the probability of selecting a green word in natural text is 50% (i.e., \\(P(\\text{Green}) = 0.5\\)). We can analyze the observed frequency of green words in \\(m\\) trials (words in the text) to determine if the text is likely watermarked.\nThe probability of observing exactly \\(K\\) green words in \\(m\\) trials follows a binomial distribution:\nP (k success in m trials) = [ (Pr())^K (1 - Pr())^{m-K}, ]\nwhere \\(m\\) \\(K\\) is the binomial coefficient.\nBy comparing the observed frequency of green words to the expected frequency under the null hypothesis (50/50 distribution), we can infer whether the text is watermarked.\nThe full summation form of this probability is: P (k success in m trials) = [ _{l=k}^m p^l (1 - p)^{m-l} ]\nwhere \\(p\\) is the probability of success (e.g., selecting a green word).\nModifying the probability distribution in watermarking slightly affects the output quality. Selecting a word that is not the highest probability increases the risk of hallucinations since LLMs are auto-generative and build text sequentially.\nAdditionally, if someone gains access to the model, they could potentially reverse-engineer the watermarking process by distinguishing red words from green words. For example, the word “delve” could be identified as a green word generated by ChatGPT, providing evidence of watermarking being used."
  },
  {
    "objectID": "notes/08_watermarking.html#distortion-free-watermarking",
    "href": "notes/08_watermarking.html#distortion-free-watermarking",
    "title": "Watermarking LLMs",
    "section": "Distortion-Free Watermarking",
    "text": "Distortion-Free Watermarking\nTo solve the issue of modifying the probability distribution, distortion-free watermarking provides a method to embed a watermark without altering the output probabilities significantly. For example, consider the sentence:\n“Vermont kale in the winter! ___”\nIn this approach:\n\nThe tokens within a designated window, let’s say the last three words—“in”, “the”, “winter” for this example—are converted into numerical values and summed to create a seed.\nThis seed is a multiplier to generate a random yet replicable sampling from the probability distribution of the next word without introducing distortion.\n\nTo detect this watermark, one would:\n\nAccess the prompt or the preceding context (e.g., “Vermont kale in the winter!”).\nExtract the last three tokens, regenerate the seed, and sample from the given probability distribution.\nCompare the generated probability distribution with the observed text.\n\nThis approach ensures the watermark is detectable without modifying the core probability distribution of the language model; however, it is still an unreasonable solution because it requires the model to have access to the prompt as well."
  },
  {
    "objectID": "notes/08_watermarking.html#exponential-minimum-sampling",
    "href": "notes/08_watermarking.html#exponential-minimum-sampling",
    "title": "Watermarking LLMs",
    "section": "Exponential Minimum Sampling",
    "text": "Exponential Minimum Sampling\nExponential Minimum Sampling addresses the core challenges of watermarking: it does not require access to the prompt and avoids altering the probability distribution. Here, a pseudo-random vector \\(X\\) is generated using a seed derived from a sliding window of the given text. This \\(X\\) vector has continuous values between 0 and 1, with a length equal to the dimensionality of the vocabulary, \\(|v|\\).\nThe next token is selected based on minimizing the cost function:\n\\(\\text{Cost} = -\\frac{\\log(x_i)}{\\text{len(token\\_ids)}},\\)\nwhere \\(x_i\\) are entries from the \\(X\\) vector. This method ensures that the probability of selecting a specific token \\(i^*\\) corresponds directly to its true likelihood \\(P(i^*)\\) in the original probability distribution.\n\nPseudo-code for Detection:\ncost = 0\nfor token_id in token_ids:\n    seed = calculate_seed(token_id, window)\n    x_vector = generate_x_vector(seed)\n    cost += -log(x_vector[token_id]) / len(token_ids)\nThe probability of selecting the next word can be expressed as:\n\\(\\text{next} = \\arg\\min_i \\frac{-\\log x_i}{p_i},\\) where \\(x \\sim \\text{Uniform}(0, 1)^|v|\\).\nAdditionally we know the probobility of selecting the next word, i, is just P(i) because:\n\\(P\\left(-\\frac{\\log x_i}{p_i} \\geq t\\right) = P\\left(x_i \\leq e^{-p_i t}\\right) = e^{-p_i t}.\\)\nThus, the probability density of \\(-\\frac{\\log x_i}{p_i}\\) is: \\(P\\left(-\\frac{\\log x_i}{p_i} = u\\right) = p_i \\cdot e^{-p_i u}.\\)\nThe cumulative probability of the selected word being \\(\\arg\\min_i \\frac{-\\log x_i}{p_i}\\) is: \\(P\\left(i^* = \\arg\\min_i \\frac{-\\log x_i}{p_i}\\right) =\n\\int_{u=t}^\\infty P\\left(-\\frac{\\log x_i}{p_i} = u\\right)\n\\prod_{j \\neq i} P\\left(-\\frac{\\log x_j}{p_j} &gt; t\\right) \\, du.\\)\nSimplifying further: \\(P\\left(i^* = \\arg\\min_i \\frac{-\\log x_i}{p_i}\\right) =\np_i \\int_{u=t}^\\infty e^{-p_i u} \\prod_{j \\neq i} e^{-p_j u} \\, du.\\)\nExpanding the product term:\n\\(P\\left(i^* = \\arg\\min_i \\frac{-\\log x_i}{p_i}\\right) =\np_i \\int_{u=t}^\\infty e^{-\\left(\\sum_{j} p_j\\right) u} \\, du.\\)\nFinally, solving the integral:\n\\(P\\left(i^* = \\arg\\min_i \\frac{-\\log x_i}{p_i}\\right) =\np_i \\cdot \\left[-\\frac{1}{\\sum_{j} p_j} e^{-\\left(\\sum_{j} p_j\\right) u}\\right]_t^\\infty,\\)\nwhich simplifies to:\n\\(P\\left(i^* = \\arg\\min_i \\frac{-\\log x_i}{p_i}\\right) = p_i.\\)\nThe seed for this process is \\(x\\)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CSCI 1051: Deep Learning",
    "section": "",
    "text": "Deep learning from theoretical foundations to state-of-the-art applications.\n\n\n\n\nInstructor: R. Teal Witter. Please call me Teal.\nClass Times: We meet Monday, Tuesday, Wednesday, and Thursday in 75 Shannon St Room 202. The lecture is from 10am to noon and the discussion is from 2 to 4pm.\nOffice Hours: I will hold office hours in 75 Shannon St Room 221 from 3 to 4pm.\nParticipation: I expect you to engage in class, ask questions, and make connections. So I can check in with you, please fill out this form every lecture.\nScribed Notes: Notes are a great resource for augmenting your in-class learning. Since I updated the material in this course, I would like to ask for your help in writing these notes. You can find the assignment here.\n\n\nAssignments: You will receive one problem per class. I expect you to solve the problem with your group during the discussion (I’ll be there to answer questions). Once you have solved the problem, you should write up your solution on your own. In addition, there will be a project on a lecture topic of your choice.\n\n\n\nAssignment\n\n\nDue\n\n\n\n\nProblem Set 1 (LaTeX)\n\n\nFriday 1/10\n\n\n\n\nProblem Set 2 (LaTeX)\n\n\nFriday 1/17\n\n\n\n\nProblem Set 3 (LaTeX)\n\n\nFriday 1/24\n\n\n\n\nProblem Set 4 (LaTeX)\n\n\nWednesday 1/29\n\n\n\n\nProject Proposal\n\n\nSunday 1/26\n\n\n\n\nProject\n\n\nFriday 1/31\n\n\n\n\n\n\n\n\nClass\n\n\nTopic\n\n\nSlides\n\n\nResources\n\n\n\n\nThe Three-Step Framework\n\n\n\n\nMonday 1/6\n\n\nLinear Regression and Mean Squared Error\n\n\nSlides\n\n\nMy Notes\n\n\n\n\nTuesday 1/7\n\n\nLogistic Regression and Cross Entropy Loss\n\n\nSlides\n\n\nStudent Notes\n\n\n\n\nWednesday 1/8\n\n\nGradient Descent and Neural Networks\n\n\nSlides\n\n\nChinmay’s Notes\n\n\n\n\nThursday 1/9\n\n\nBack-propagation and Optimization\n\n\nSlides\n\n\nChinmay’s Notes\n\n\n\n\nLanguage Generation\n\n\n\n\nMonday 1/13\n\n\nLanguage Embeddings and Contrastive Learning\n\n\nSlides\n\n\n\n\n\n\nTuesday 1/14\n\n\nTransformers and Positional Encoding\n\n\nSlides\n\n\nStudent Notes\n\n\n\n\nWednesday 1/15\n\n\nLow Rank Approximation\n\n\nSlides\n\n\n\n\n\n\nThursday 1/16\n\n\nWatermarking\n\n\nSlides\n\n\nStudent Notes\n\n\n\n\nImage Generation\n\n\n\n\nTuesday 1/21\n\n\nConvolutional Neural Networks and Image Embeddings\n\n\nSlides\n\n\n\n\n\n\nWednesday 1/22\n\n\nDiffusion\n\n\nSlides\n\n\n\n\n\n\nThursday 1/23\n\n\nSchrödinger Bridges and CLIP\n\n\nSlides\n\n\n\n\n\n\nAI Safety\n\n\n\n\nMonday 1/27\n\n\nInterpretability\n\n\nSlides\n\n\n\n\n\n\nTuesday 1/28\n\n\nProject Preparation\n\n\n\n\n\n\n\n\nWednesday 1/29\n\n\nProject Preparation\n\n\n\n\n\n\n\n\nThursday 1/30\n\n\nProject Presentations"
  },
  {
    "objectID": "notes/code.html",
    "href": "notes/code.html",
    "title": "Code for Producing Images in Lecture Notes",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport numpy as np\nfrom mpl_toolkits.mplot3d import Axes3D\n\n\nLinear Regression Figures\n\nnp.random.seed(1234) # Seed randomness\n\nn = 10 # Number of observations\nw = 2 # True parameter\nX = np.random.rand(n) # x-values\ny = X.dot(w).T + np.random.normal(size=n) * .2 #y-values\n\nplt.scatter(X,y, color='black', label=r'Data: $(x^{(i)}, y^{(i)})$')\nplt.xlabel(r'$x$')\nplt.ylabel(r'$y$')\nxaxis = np.arange(0,1,.01)\nplt.plot(xaxis, xaxis*.5, label=r'Line: $f(x) = .5x$', color='red')\nplt.plot(xaxis, xaxis*w, label=r'Line: $f(x) = 2x$', color='green')\nplt.legend()\nplt.title(r'Linear Regression in $\\mathbb{R}^1$')\nplt.savefig('images/regression_1d.pdf')\n\n\n---------------------------------------------------------------------------\nFileNotFoundError                         Traceback (most recent call last)\n&lt;ipython-input-2-d86d24ec903a&gt; in &lt;cell line: 16&gt;()\n     14 plt.legend()\n     15 plt.title(r'Linear Regression in $\\mathbb{R}^1$')\n---&gt; 16 plt.savefig('images/regression_1d.pdf')\n\n/usr/local/lib/python3.10/dist-packages/matplotlib/pyplot.py in savefig(*args, **kwargs)\n   1117     # savefig default implementation has no return, so mypy is unhappy\n   1118     # presumably this is here because subclasses can return?\n-&gt; 1119     res = fig.savefig(*args, **kwargs)  # type: ignore[func-returns-value]\n   1120     fig.canvas.draw_idle()  # Need this if 'transparent=True', to reset colors.\n   1121     return res\n\n/usr/local/lib/python3.10/dist-packages/matplotlib/figure.py in savefig(self, fname, transparent, **kwargs)\n   3388                 for ax in self.axes:\n   3389                     _recursively_make_axes_transparent(stack, ax)\n-&gt; 3390             self.canvas.print_figure(fname, **kwargs)\n   3391 \n   3392     def ginput(self, n=1, timeout=30, show_clicks=True,\n\n/usr/local/lib/python3.10/dist-packages/matplotlib/backend_bases.py in print_figure(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\n   2185                 # force the figure dpi to 72), so we need to set it again here.\n   2186                 with cbook._setattr_cm(self.figure, dpi=dpi):\n-&gt; 2187                     result = print_method(\n   2188                         filename,\n   2189                         facecolor=facecolor,\n\n/usr/local/lib/python3.10/dist-packages/matplotlib/backend_bases.py in &lt;lambda&gt;(*args, **kwargs)\n   2041                 \"bbox_inches_restore\"}\n   2042             skip = optional_kws - {*inspect.signature(meth).parameters}\n-&gt; 2043             print_method = functools.wraps(meth)(lambda *args, **kwargs: meth(\n   2044                 *args, **{k: v for k, v in kwargs.items() if k not in skip}))\n   2045         else:  # Let third-parties do as they see fit.\n\n/usr/local/lib/python3.10/dist-packages/matplotlib/backends/backend_pdf.py in print_pdf(self, filename, bbox_inches_restore, metadata)\n   2798             file = filename._ensure_file()\n   2799         else:\n-&gt; 2800             file = PdfFile(filename, metadata=metadata)\n   2801         try:\n   2802             file.newPage(width, height)\n\n/usr/local/lib/python3.10/dist-packages/matplotlib/backends/backend_pdf.py in __init__(self, filename, metadata)\n    686         self.original_file_like = None\n    687         self.tell_base = 0\n--&gt; 688         fh, opened = cbook.to_filehandle(filename, \"wb\", return_opened=True)\n    689         if not opened:\n    690             try:\n\n/usr/local/lib/python3.10/dist-packages/matplotlib/cbook.py in to_filehandle(fname, flag, return_opened, encoding)\n    481             fh = bz2.BZ2File(fname, flag)\n    482         else:\n--&gt; 483             fh = open(fname, flag, encoding=encoding)\n    484         opened = True\n    485     elif hasattr(fname, 'seek'):\n\nFileNotFoundError: [Errno 2] No such file or directory: 'images/regression_1d.pdf'\n\n\n\n\n\n\n\n\n\n\n\nplt.xlabel(r'$z$')\nplt.ylabel(r'$\\mathcal{L}(z)$')\nxaxis = np.arange(-1.5,1.5,.001)\nplt.plot(xaxis, xaxis**2, label=r'Squared Loss: $\\mathcal{L}(z)=z^2$', color='blue')\nplt.plot(xaxis, np.abs(xaxis), label=r'Absolute Loss: $\\mathcal{L}(z)=|z|$', color='purple', linestyle='dotted')\nplt.legend()\nplt.title(r'Squared and Absolute Losses')\nplt.savefig('images/regression_losses.pdf')\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.lines import Line2D\n\n# Seed randomness\nnp.random.seed(1234)\nn = 10  # Number of observations\nw = np.array([2, .5])  # True parameter\nX = np.random.rand(n, 2)  # x-values\ny = X.dot(w).T + np.random.normal(size=n) * .1  # y-values\n\n# Create figure and 3D axis\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\n\n# Scatter plot for data points\nax.scatter(X[:, 0], X[:, 1], y, color='black', label=r'Data: $(x_1^{(i)}, x_2^{(i)}, y^{(i)})$')\n\n# Hyperplane 1: Green\nx1 = np.arange(0, 1, .01)\nx2 = np.arange(0, 1, .01)\nX1, X2 = np.meshgrid(x1, x2)\nZ = w[0] * X1 + w[1] * X2\nax.plot_surface(X1, X2, Z, alpha=.5, color='green')\n\n# Hyperplane 2: Red\nax.plot_surface(X1, X2, .5 * X1 + 0 * X2, alpha=.5, color='red')\n\n# Labels and title\nax.set_xlabel(r'$x_1$')\nax.set_ylabel(r'$x_2$')\nax.set_zlabel(r'$y$')\nax.set_title(r'Linear Regression in $\\mathbb{R}^2$')\n\n# Manually create custom legend handles for the surfaces\nhandles = [\n    Line2D([0], [0], marker='o', color='black', markerfacecolor='black', markersize=6, label=r'Data: $(x_1^{(i)}, x_2^{(i)}, y^{(i)})$'),\n    Line2D([0], [0], color='green', lw=4, label=r'Hyperplane: $f(x) = 2x_1 + .5x_2$'),\n    Line2D([0], [0], color='red', lw=4, label=r'Hyperplane: $f(x) = .5x_1 + 0x_2$')\n]\n\n# Add legend\nplt.legend(handles=handles, loc='upper left', framealpha=1)\n\n# Save the figure\nplt.savefig('images/regression_2d.pdf', bbox_inches='tight')\nplt.show()\n\n\n\nLogistic Regression Figures\n\nplt.xlabel(r'$z$')\n#plt.ylabel(r'$\\sigma(z)$')\nxaxis = np.arange(-10,10,.001)\nsigma = lambda z : 1 / (1+np.exp(-z))\nplt.plot(xaxis, sigma(xaxis), label=r'$\\sigma(z)$', color='blue')\nplt.legend()\nplt.title(r'Sigmoid Function')\nplt.savefig('images/logistic_sigmoid.pdf')\n\n\nplt.xlabel(r'$z$')\n#plt.ylabel(r'$\\sigma(z)$')\nxaxis = np.arange(-10,10,.001)\nsigma = lambda z : 1 / (1+np.exp(-z))\nplt.plot(xaxis, sigma(xaxis), label=r'$\\sigma(z)$', color='blue')\nplt.legend()\nplt.title(r'Sigmoid Function')\nplt.savefig('images/logistic_sigmoid.pdf')\n\n\n# Create data points\nz = np.linspace(0.1, 5, 1000)  # Avoid z=0 since ln(0) is undefined\ny = -np.log(z)\n\n# Create the figure and axis\nplt.figure(figsize=(10, 6))\nplt.plot(z, y, 'b-', linewidth=2, label='-ln(z)')\n\n# Add grid\nplt.grid(True, linestyle='--', alpha=0.7)\n\n# Add title and labels\nplt.title('Graph of -ln(z)', fontsize=14)\nplt.xlabel('z', fontsize=12)\nplt.ylabel('-ln(z)', fontsize=12)\n\n# Add legend\nplt.legend(fontsize=12)\n\n# Add horizontal and vertical axes\nplt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\nplt.axvline(x=0, color='k', linestyle='-', alpha=0.3)\n\n# Set reasonable axis limits\nplt.xlim(0, 5)\nplt.ylim(-2, 3)\n\n# Adjust layout\nplt.tight_layout()\n\nplt.savefig('images/NegLog.pdf')\n# Show plot\nplt.show()\n\n\n# Create positional encoding of time example graph\n# Create data points\nimport os\nz = np.linspace(-4, 4, 1000)\ny1 = np.sin(z*4)+3\ny2 = np.sin(z*8)+1\ny3 = np.sin(z*16)-1\ny4 = np.sin(z*32)-3\ny5 = np.sin(z-2)*10000\ny6 = np.sin(z-3)*10000\n\n\n# Create the figure and axis\nplt.figure(figsize=(10, 6))\nplt.plot(z, y1, linewidth=2, label='months', color=\"#387490\")\n\nplt.plot(z, y2, linewidth=2, label='days', color=\"#89B374\")\n\nplt.plot(z, y3, linewidth=2, label='hours', color=\"#702E52\")\n\nplt.plot(z, y4, linewidth=2, label='minutes', color=\"#FEBE28\")\n\nplt.plot(z, y5, linewidth=4, label=\"t\", color='#F2503B')\n\nplt.plot(z, y6, linewidth=4, label=\"t+1\", color='#F2503B')\n\n# Add grid\nplt.grid(True, linestyle='--', alpha=0.7)\n\n# Add title and labels\nplt.title('Positional Encoding Example', fontsize=14)\nplt.xlabel('Time', fontsize=12)\n\n#annotate t values\nplt.annotate(\"t\", (2.1, 2), fontsize=20)\nplt.annotate(\"t+1\", (3.1, 2), fontsize=20)\n\n\n# Add horizontal and vertical axes\nplt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\nplt.axvline(x=0, color='k', linestyle='-', alpha=0.3)\n\n# Set reasonable axis limits\nplt.xlim(0, 4)\nplt.ylim(-4, 4)\n\nplt.xticks(None)\n\n# Adjust layout\nplt.tight_layout()\n\nplt.axis('off')\n\nplt.savefig('images/PositionalEncodingOfTime.pdf')\n# Show plot\nplt.show()\n\n/content"
  },
  {
    "objectID": "notes/06_TransformersPositionalEncoding.html",
    "href": "notes/06_TransformersPositionalEncoding.html",
    "title": "Transformers and Positional Encoding",
    "section": "",
    "text": "Written by Francis Cataldo and Thomas Normand"
  },
  {
    "objectID": "notes/06_TransformersPositionalEncoding.html#motivation",
    "href": "notes/06_TransformersPositionalEncoding.html#motivation",
    "title": "Transformers and Positional Encoding",
    "section": "Motivation",
    "text": "Motivation\n\nGiven the example sentence: the cow jumped over the moon. Convert the sentence into an nxd tensor (n is the number of words, d is the vector length) where each words is one-hot encoded into a vector.\n\n\nAnother example sentence: the cat plays the fiddle\n\n\nConvert the sentence into an mxd tensor (m is the number of words, d is vector length). Now, we want the following: takes sequences of varying lengths, accounts for short range and long range dependencies, and considers context of words (i.e. fiddle with door vs. fiddle the instrument).\nAttention is the solution… We will consider two versions of attention: Self-Attention and Cross-Attention"
  },
  {
    "objectID": "notes/06_TransformersPositionalEncoding.html#self-attention",
    "href": "notes/06_TransformersPositionalEncoding.html#self-attention",
    "title": "Transformers and Positional Encoding",
    "section": "Self-Attention",
    "text": "Self-Attention\n\nThe self-attention layer takes in one-hot encodings of words and returns a linear combination of the inputted vectors\n\n\nNote: the subsequent stacked layers take in linear combinations of vectors and return probability distributions\n\n\nAt the end, the vectors are passed into another trained aspect of the model that takes in the linear combination of vectors from the previous attention layers and returns a probability distribution of the next word\n\n\n\n\n\nThis model is implemented via matrix multiplications simplified into the following diagram.\n\n\n\n\nHere, the “W” matrices are simply trained weights and the “X” matrix is the inputted words. The “X” matrix is broken up into queries, keys, and values that are run through the matrix multiplications. After, they are passed through the softmax to create a probability distribution\n\nWe begin by establishing that each predicted vector \\(y^{(i)}\\) should be a linear combination of the other words in the vocabulary. That is, \\(y^{(i)}=\\sum_{i=1}^n x^{i}\\text{sim}(i,j)\\) One way to calculate the similarity between the ith and jth words is softmax.\nHowever, we aren’t fully leveraging the power of deep learning. Note that since attention models are fully connected, they can be thought of in terms of matrix multiplication. This motivates a modified approach.\nIncorporating a weight matrix \\(W^{(v)}\\) we write,\n\\(y^{(i)}=\\sum_{i=1}^n W^{(v)}x^{i}\\text{soft}(&lt;W^{(k)}x^{(i)},W^{(q)}x^{(j)}&gt;)\\) \\(\\Rightarrow W^{(v)}\\sum_{i=1}^n x^{i}\\text{soft}(x^{(i)^T}W^{(k)^T}W^{(q)}x^{(j)})\\)\nWe’ve encountered an issue: computing the outputs in this way is extremely time consuming. How can we refine this approach?\nLet \\(X\\) be the matrix containing the vector embeddings of each word and consider the \\(W^{(q)}x^{(j)}\\) term. Note that by indexing the jth row of the resulting matrix of \\(XW^{(q)^T}\\) we obtain \\(W^{(q)}x^{(j)}\\). Similarly, if we index the jth column of \\((XW^{(k)^T})^T\\) we can find \\(W^{(k)}x^{(i)}\\). Then, because softmax produces a vector, call it \\(v^{(i)}\\), we can rewrite this as,\n\\(W^{(v)}\\sum_{i=1}^n x^{i}\\text{soft}(x^{(i)^T}W^{(k)^T}W^{(q)}x^{(j)})\\) \\(\\Rightarrow \\text{soft}(XW^{(q)^T}W^{(k)^T}X)XW^{(v)}\\)\nWe now have a succinct way of representing the desired outputs in terms of matrix multiplication, a task modern GPUs are very good at. By indexing the jth row of this resulting matrix, we can obtain \\(y^{(j)}\\).\n\n\nSelf-attention got its name because the queries and keys are the same\nSelf-attention layers are stacked on top of each other\n\nSome models use as many as 96 layers in sequence. Also possible to utilize them with multiple self-attentions in one layer.\n\nAt the end, a sequence of vectors is returned that can be interpreted as a probability distribution of the next word.\nNext, the value should be interpreted as a probability. Training is required to improve this.\nThis process is achieved by repeatedly testing the model against known text with select words removed. The model then predicts the word and is evaluated via the cross entropy loss function and then, the weights are updated."
  },
  {
    "objectID": "notes/06_TransformersPositionalEncoding.html#cross-attention",
    "href": "notes/06_TransformersPositionalEncoding.html#cross-attention",
    "title": "Transformers and Positional Encoding",
    "section": "Cross-Attention",
    "text": "Cross-Attention\n\nModivation: Cross-Attention allows for the translation of meaning between languages. It differs from Self-Attention because Cross-Attention analyzes the connections between two sequences and Self-Attention analyzes only one sequence.\nIn implementation, Cross-Attention is very similar to Self-Attention with the small tweaks highlighted on the following diagram"
  },
  {
    "objectID": "notes/06_TransformersPositionalEncoding.html#positional-encoding",
    "href": "notes/06_TransformersPositionalEncoding.html#positional-encoding",
    "title": "Transformers and Positional Encoding",
    "section": "Positional Encoding",
    "text": "Positional Encoding\n\nModivation: Consider the time representation 1,065,066,880 minutes since 0 BC. This is not easily understandable. In practice, instead of this, we encode this time as 11:24am Tuesday, Jan 14, 2025. This is an example of positional encoding. The minute, day, and date are all useful for scheduling. The hour attribute captures the time of day, month captures the time of year, and year counts the years past.\n\n\n\n\n\nA certain time can be encoded by drawing a line from through the different sinusodial fucntions, like the red line shown above. These values can be thought of as the result of different sinusodial functions. All of this culminates to a much more readable value. This concept will be utilized in machine learning by encoding strings in a similar manner in a subsequent lecture.\nIn practice, we add on this new information to the bottom of each of the one-hot encoded vectors and access it later to retrieve the positional information. In true LLMs, self-attention and cross attention layers are implemented in series and in parallel, each focusing on different elements of the input text."
  },
  {
    "objectID": "notes/01_regression.html",
    "href": "notes/01_regression.html",
    "title": "Linear Regression and Mean Squared Error",
    "section": "",
    "text": "When I first heard about machine learning, I imagined a computer that was rewarded every time it gave the right answer. Maybe there were electric carrots and sticks that no one had bothered to tell me about? While I now know as little as I did then about computer hardware, I have learned that machine learning is fundamentally a mathematical process.\nLuckily, we’ve been learning about the very mathematical ideas that make machine learning work for years! We’ll review the basics of these concepts and then jump in to linear regression, arguably the foundation of neural networks.\n\n\nImagine a function \\(\\mathcal{L}: \\mathbb{R} \\to \\mathbb{R}\\). (Instead of the usual \\(f\\), we’ll use \\(\\mathcal{L}\\) for reasons that will soon become clear.) The mapping notation means that \\(\\mathcal{L}\\) takes a single real number as input and outputs a single real number. In general, mathematicians tell us to be careful about whether we can differentiate a function. But, we’re computer scientists so we’ll risk it for the biscuit.\nLet \\(z \\in \\mathbb{R}\\) be the input to \\(\\mathcal{L}\\). The derivative of \\(\\mathcal{L}\\) with respect to its input \\(z\\) is mathematically denoted by \\(\\frac{\\partial}{\\partial z}[\\mathcal{L}(z)]\\).\nFormally, the derivative is defined as \\[\n\\frac{\\partial}{\\partial z}[\\mathcal{L}(z)]\n= \\lim_{h \\to 0} \\frac{\\mathcal{L}(z + h) - \\mathcal{L}(z)}{h}.\n\\] If we were to plot \\(\\mathcal{L}\\), the derivative at a point \\(z\\) would be the slope of the tangent line to the curve at that point.\nHere are several examples of functions and their derivatives that you might remember from calculus.\n\n\n\nFunction: \\(\\mathcal{L}(z)\\) \n\n\nDerivative: \\(\\frac{\\partial}{\\partial z}[\\mathcal{L}(z)]\\)\n\n\n\n\n\\[z^2\\]\n\n\n\\[2z\\]\n\n\n\n\n\\[z^a\\]\n\n\n\\[a z^{a-1}\\]\n\n\n\n\n\\[az + b\\]\n\n\n\\[a\\]\n\n\n\n\n\\[\\ln(z)\\]\n\n\n\\[\\frac{1}{z}\\]\n\n\n\n\n\\[e^z\\]\n\n\n\\[e^z\\]\n\n\n\n\n\n\nWhile working with a simple basic function is easy, we’re not always so lucky. Modern machine learning chains many many complicated functions together. Fortunately, we will think of these operations modularly.\nLet \\(g: \\mathbb{R} \\to \\mathbb{R}\\) be another function. Consider the composite function \\(g(\\mathcal{L}(z))\\).\nBy the chain rule, the derivative of \\(g(\\mathcal{L}(z))\\) with respect to \\(z\\) is \\[\n\\frac{\\partial }{\\partial z}[g(\\mathcal{L}(z))]\n= \\frac{\\partial g}{\\partial z}(\\mathcal{L}(z))\n\\frac{\\partial}{\\partial z}[\\mathcal{L}(z)].\n\\]\nOften, we will also multiply functions together. The product rule tells us that \\[\n\\frac{\\partial }{\\partial z}[g(z) \\mathcal{L}(z)]\n= g(z) \\frac{\\partial}{\\partial z}[\\mathcal{L}(z)]\n+ \\mathcal{L}(z) \\frac{\\partial}{\\partial z}[g(z)].\n\\]\n\n\n\nIn machine learning, we process high-dimensional data so we are interested in functions with multivariate input. Consider \\(\\mathcal{L}: \\mathbb{R}^d \\to \\mathbb{R}\\). The output of the function is still a real number but the input consists of \\(d\\) real numbers. We will use the vector \\(\\mathbf{z} \\in \\mathbb{R}^d\\) to represent all \\(d\\) inputs \\(z_1, \\ldots, z_d\\).\nInstead of the derivative, we will talk use the partial derivative. The partial derivative with respect to \\(z_i\\) is denoted by \\(\\frac{\\partial}{\\partial z_i}[\\mathcal{L}(\\mathbf{z})]\\). In effect, the partial derivative tells us how \\(\\mathcal{L}\\) changes when we change \\(z_i\\) while keeping all other inputs fixed.\nThe gradient stores all the partial derivatives in a vector. The \\(i\\)th entry of this vector is given by the partial derivative of \\(\\mathcal{L}\\) with respect to \\(z_i\\). In mathematical notation, \\[\n\\nabla_\\mathbf{z} \\mathcal{L} = \\left[\\begin{smallmatrix} \\frac{\\partial}{\\partial z_1}[\\mathcal{L}(\\mathbf{z})] \\\\ \\vdots \\\\ \\frac{\\partial}{\\partial z_d}[\\mathcal{L}(\\mathbf{z})] \\\\ \\end{smallmatrix}\\right]\n\\]\nJust like the derivative in one dimension, the gradient contains information about the slope of \\(\\mathcal{L}\\) with respect to each of the \\(d\\) dimensions in its input.\n\n\n\nVector and matrix multiplication lives at the heart of deep learning. In fact, deep learning really started to take off when researchers realized that the Graphical Processing Unit (GPU) could be used to perform gradient updates in addition to the matrix multiplication it was designed to do for gaming.\nConsider two vectors \\(\\mathbf{u} \\in \\mathbb{R}^d\\) and \\(\\mathbf{v} \\in \\mathbb{R}^d\\). We will use \\(\\mathbf{u} \\cdot \\mathbf{v} = \\sum_{i=1}^d u_i v_i\\) to denote the inner product of \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\). The \\(\\mathcal{\\ell}_2\\)-norm of \\(v\\) is given by \\(\\|\\mathbf{v}\\|_2 = \\sqrt{\\mathbf{u} \\cdot \\mathbf{u}}\\).\nConsider two matrices: \\(\\mathbf{A} \\in \\mathbb{R}^{d \\times m}\\) and \\(\\mathbf{B} \\in \\mathbb{R}^{m \\times n}\\) where \\(d \\neq n\\). We can only multiply two matrices when their inner dimension agrees. Because the number of columns in \\(\\mathbf{A}\\) is the same as the number of rows in \\(\\mathbf{B}\\), we can compute \\(\\mathbf{AB}\\). However, because the number of columns in \\(\\mathbf{B}\\) is not the same as the number of rows in \\(\\mathbf{A}\\), the product \\(\\mathbf{BA}\\) is not defined.\nWhen we can multiply two matrices, the \\((i,j)\\) entry in \\(\\mathbf{AB}\\) is given by the inner product between the \\(i\\)th row of \\(\\mathbf{A}\\) and the \\(j\\)th column of \\(\\mathbf{B}\\). The resulting dimensions of the matrix product will be the number of rows in \\(\\mathbf{A}\\) by the number of columns in \\(\\mathbf{B}\\).\n\n\n\nIf we have a scalar equation \\(ax = b\\), we can simply solve for \\(x\\) by dividing both sides by \\(a\\). In effect, we are applying the inverse of \\(a\\) to \\(a\\) i.e., \\(\\frac1{a} a =1\\). The same principle applies to matrices. The \\(n \\times n\\) identity matrix generalizes the scalar identity \\(1\\). This identity matrix is denoted by \\(\\mathbf{I}_{n \\times n} \\in \\mathbb{R}^{n \\times n}\\): the on-diagonal entries \\((i,i)\\) are 1 while the off-diagonal entries \\((i,j)\\) for \\(i\\neq j\\) are 0.\nConsider the matrix equation \\(\\mathbf{Ax} = \\mathbf{b}\\) where \\(\\mathbf{A} \\in \\mathbb{R}^{d \\times d}\\), \\(\\mathbf{x} \\in \\mathbb{R}^d\\), and \\(\\mathbf{b} \\in \\mathbb{R}^d\\). (Notice that the inner dimensions of \\(\\mathbf{A}\\) and \\(\\mathbf{x}\\) agree so their multiplication is well-defined, and the resulting vector is the same dimension as \\(\\mathbf{b}\\).)\nIf we want to solve for \\(\\mathbf{x}\\), we can use the matrix inverse. For a matrix \\(\\mathbf{A}\\), we use \\(\\mathbf{A}^{-1}\\) to denote its inverse. The inverse is defined so that \\(\\mathbf{A}^{-1} \\mathbf{A} = \\mathbf{I}_{n \\times n}\\) where \\(\\mathbf{I}_{n \\times n}\\) is the identity matrix. Then, we can solve for \\(\\mathbf{x}\\) by multiplying both sides of the equation by \\(\\mathbf{A}^{-1}\\). \\[\n\\mathbf{A}^{-1} \\mathbf{Ax} = \\mathbf{A}^{-1} \\mathbf{b}\n\\] Since \\(\\mathbf{A}^{-1} \\mathbf{A} = \\mathbf{I}_{n \\times n}\\), we have that \\(\\mathbf{I}_{n \\times n} \\mathbf{x} = \\mathbf{x} = \\mathbf{A}^{-1} \\mathbf{b}\\)."
  },
  {
    "objectID": "notes/01_regression.html#math-review",
    "href": "notes/01_regression.html#math-review",
    "title": "Linear Regression and Mean Squared Error",
    "section": "",
    "text": "When I first heard about machine learning, I imagined a computer that was rewarded every time it gave the right answer. Maybe there were electric carrots and sticks that no one had bothered to tell me about? While I now know as little as I did then about computer hardware, I have learned that machine learning is fundamentally a mathematical process.\nLuckily, we’ve been learning about the very mathematical ideas that make machine learning work for years! We’ll review the basics of these concepts and then jump in to linear regression, arguably the foundation of neural networks.\n\n\nImagine a function \\(\\mathcal{L}: \\mathbb{R} \\to \\mathbb{R}\\). (Instead of the usual \\(f\\), we’ll use \\(\\mathcal{L}\\) for reasons that will soon become clear.) The mapping notation means that \\(\\mathcal{L}\\) takes a single real number as input and outputs a single real number. In general, mathematicians tell us to be careful about whether we can differentiate a function. But, we’re computer scientists so we’ll risk it for the biscuit.\nLet \\(z \\in \\mathbb{R}\\) be the input to \\(\\mathcal{L}\\). The derivative of \\(\\mathcal{L}\\) with respect to its input \\(z\\) is mathematically denoted by \\(\\frac{\\partial}{\\partial z}[\\mathcal{L}(z)]\\).\nFormally, the derivative is defined as \\[\n\\frac{\\partial}{\\partial z}[\\mathcal{L}(z)]\n= \\lim_{h \\to 0} \\frac{\\mathcal{L}(z + h) - \\mathcal{L}(z)}{h}.\n\\] If we were to plot \\(\\mathcal{L}\\), the derivative at a point \\(z\\) would be the slope of the tangent line to the curve at that point.\nHere are several examples of functions and their derivatives that you might remember from calculus.\n\n\n\nFunction: \\(\\mathcal{L}(z)\\) \n\n\nDerivative: \\(\\frac{\\partial}{\\partial z}[\\mathcal{L}(z)]\\)\n\n\n\n\n\\[z^2\\]\n\n\n\\[2z\\]\n\n\n\n\n\\[z^a\\]\n\n\n\\[a z^{a-1}\\]\n\n\n\n\n\\[az + b\\]\n\n\n\\[a\\]\n\n\n\n\n\\[\\ln(z)\\]\n\n\n\\[\\frac{1}{z}\\]\n\n\n\n\n\\[e^z\\]\n\n\n\\[e^z\\]\n\n\n\n\n\n\nWhile working with a simple basic function is easy, we’re not always so lucky. Modern machine learning chains many many complicated functions together. Fortunately, we will think of these operations modularly.\nLet \\(g: \\mathbb{R} \\to \\mathbb{R}\\) be another function. Consider the composite function \\(g(\\mathcal{L}(z))\\).\nBy the chain rule, the derivative of \\(g(\\mathcal{L}(z))\\) with respect to \\(z\\) is \\[\n\\frac{\\partial }{\\partial z}[g(\\mathcal{L}(z))]\n= \\frac{\\partial g}{\\partial z}(\\mathcal{L}(z))\n\\frac{\\partial}{\\partial z}[\\mathcal{L}(z)].\n\\]\nOften, we will also multiply functions together. The product rule tells us that \\[\n\\frac{\\partial }{\\partial z}[g(z) \\mathcal{L}(z)]\n= g(z) \\frac{\\partial}{\\partial z}[\\mathcal{L}(z)]\n+ \\mathcal{L}(z) \\frac{\\partial}{\\partial z}[g(z)].\n\\]\n\n\n\nIn machine learning, we process high-dimensional data so we are interested in functions with multivariate input. Consider \\(\\mathcal{L}: \\mathbb{R}^d \\to \\mathbb{R}\\). The output of the function is still a real number but the input consists of \\(d\\) real numbers. We will use the vector \\(\\mathbf{z} \\in \\mathbb{R}^d\\) to represent all \\(d\\) inputs \\(z_1, \\ldots, z_d\\).\nInstead of the derivative, we will talk use the partial derivative. The partial derivative with respect to \\(z_i\\) is denoted by \\(\\frac{\\partial}{\\partial z_i}[\\mathcal{L}(\\mathbf{z})]\\). In effect, the partial derivative tells us how \\(\\mathcal{L}\\) changes when we change \\(z_i\\) while keeping all other inputs fixed.\nThe gradient stores all the partial derivatives in a vector. The \\(i\\)th entry of this vector is given by the partial derivative of \\(\\mathcal{L}\\) with respect to \\(z_i\\). In mathematical notation, \\[\n\\nabla_\\mathbf{z} \\mathcal{L} = \\left[\\begin{smallmatrix} \\frac{\\partial}{\\partial z_1}[\\mathcal{L}(\\mathbf{z})] \\\\ \\vdots \\\\ \\frac{\\partial}{\\partial z_d}[\\mathcal{L}(\\mathbf{z})] \\\\ \\end{smallmatrix}\\right]\n\\]\nJust like the derivative in one dimension, the gradient contains information about the slope of \\(\\mathcal{L}\\) with respect to each of the \\(d\\) dimensions in its input.\n\n\n\nVector and matrix multiplication lives at the heart of deep learning. In fact, deep learning really started to take off when researchers realized that the Graphical Processing Unit (GPU) could be used to perform gradient updates in addition to the matrix multiplication it was designed to do for gaming.\nConsider two vectors \\(\\mathbf{u} \\in \\mathbb{R}^d\\) and \\(\\mathbf{v} \\in \\mathbb{R}^d\\). We will use \\(\\mathbf{u} \\cdot \\mathbf{v} = \\sum_{i=1}^d u_i v_i\\) to denote the inner product of \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\). The \\(\\mathcal{\\ell}_2\\)-norm of \\(v\\) is given by \\(\\|\\mathbf{v}\\|_2 = \\sqrt{\\mathbf{u} \\cdot \\mathbf{u}}\\).\nConsider two matrices: \\(\\mathbf{A} \\in \\mathbb{R}^{d \\times m}\\) and \\(\\mathbf{B} \\in \\mathbb{R}^{m \\times n}\\) where \\(d \\neq n\\). We can only multiply two matrices when their inner dimension agrees. Because the number of columns in \\(\\mathbf{A}\\) is the same as the number of rows in \\(\\mathbf{B}\\), we can compute \\(\\mathbf{AB}\\). However, because the number of columns in \\(\\mathbf{B}\\) is not the same as the number of rows in \\(\\mathbf{A}\\), the product \\(\\mathbf{BA}\\) is not defined.\nWhen we can multiply two matrices, the \\((i,j)\\) entry in \\(\\mathbf{AB}\\) is given by the inner product between the \\(i\\)th row of \\(\\mathbf{A}\\) and the \\(j\\)th column of \\(\\mathbf{B}\\). The resulting dimensions of the matrix product will be the number of rows in \\(\\mathbf{A}\\) by the number of columns in \\(\\mathbf{B}\\).\n\n\n\nIf we have a scalar equation \\(ax = b\\), we can simply solve for \\(x\\) by dividing both sides by \\(a\\). In effect, we are applying the inverse of \\(a\\) to \\(a\\) i.e., \\(\\frac1{a} a =1\\). The same principle applies to matrices. The \\(n \\times n\\) identity matrix generalizes the scalar identity \\(1\\). This identity matrix is denoted by \\(\\mathbf{I}_{n \\times n} \\in \\mathbb{R}^{n \\times n}\\): the on-diagonal entries \\((i,i)\\) are 1 while the off-diagonal entries \\((i,j)\\) for \\(i\\neq j\\) are 0.\nConsider the matrix equation \\(\\mathbf{Ax} = \\mathbf{b}\\) where \\(\\mathbf{A} \\in \\mathbb{R}^{d \\times d}\\), \\(\\mathbf{x} \\in \\mathbb{R}^d\\), and \\(\\mathbf{b} \\in \\mathbb{R}^d\\). (Notice that the inner dimensions of \\(\\mathbf{A}\\) and \\(\\mathbf{x}\\) agree so their multiplication is well-defined, and the resulting vector is the same dimension as \\(\\mathbf{b}\\).)\nIf we want to solve for \\(\\mathbf{x}\\), we can use the matrix inverse. For a matrix \\(\\mathbf{A}\\), we use \\(\\mathbf{A}^{-1}\\) to denote its inverse. The inverse is defined so that \\(\\mathbf{A}^{-1} \\mathbf{A} = \\mathbf{I}_{n \\times n}\\) where \\(\\mathbf{I}_{n \\times n}\\) is the identity matrix. Then, we can solve for \\(\\mathbf{x}\\) by multiplying both sides of the equation by \\(\\mathbf{A}^{-1}\\). \\[\n\\mathbf{A}^{-1} \\mathbf{Ax} = \\mathbf{A}^{-1} \\mathbf{b}\n\\] Since \\(\\mathbf{A}^{-1} \\mathbf{A} = \\mathbf{I}_{n \\times n}\\), we have that \\(\\mathbf{I}_{n \\times n} \\mathbf{x} = \\mathbf{x} = \\mathbf{A}^{-1} \\mathbf{b}\\)."
  },
  {
    "objectID": "notes/01_regression.html#univariate-linear-regression",
    "href": "notes/01_regression.html#univariate-linear-regression",
    "title": "Linear Regression and Mean Squared Error",
    "section": "Univariate Linear Regression",
    "text": "Univariate Linear Regression\nNow that we’ve reviewed the basic building blocks of machine learning, we can dive into linear regression. Linear regression is a simple and powerful tool that we will use to understand the basics of deep learning.\n\nGoal\nWe will begin our study of deep learning in the supervised setting. In this setting, we are given labelled data with input features and an outcome. Formally, we will have \\(n\\) labelled observations \\((x^{(1)}, y^{(1)}), \\ldots, (x^{(n)}, y^{(n)})\\). In general, we will have \\(y \\in \\mathbb{R}\\). For simplicity, we will assume for now that \\(x \\in \\mathbb{R}\\).\nOur goal is to process the data and learn a function that approximates the outcomes. In mathematical notation, we want to learn a function \\(f: \\mathbb{R} \\to \\mathbb{R}\\) so that \\(f(x^{(i)}) \\approx y^{(i)}\\) for the \\(n\\) labelled observations \\(i \\in \\{1,\\ldots,n\\}\\).\nBefore we dive into the specific way we will accomplish this with linear regression, let’s discuss the general deep learning framework. This three-step framework gives a flexible scaffolding that we will use to understand almost every topic in this course.\nThe three-step framework includes:\n• The Model: The function that we’ll use to process the input and produce a corresponding output.\n• The Loss: The function that measures the quality of the outputs from our model. (Without loss of generality, we will assume that lower is better.)\n• The Optimizer: The method of updating the model to improve the loss.\nWith these general concepts in mind, we’ll explore linear regression.\n\n\nLinear Model\nAs its name suggests, linear regression uses a linear model to process the input into an approximation of the output. Let \\(w \\in \\mathbb{R}\\) be a weight parameter. The linear model (for one-dimensional inputs) is given by \\(f(x) = wx\\).\nUnlike many deep learning models, we can visualize the linear model since it is given by a line. In the plot, we have the \\(n=10\\) data points plotted in 2 dimensions. There is one linear model \\(f(x) = 2x\\) that closely approximates the data and another linear model \\(f(x)=\\frac12 x\\) that does not approximate the data.\n\n\n\nOur goal is to learn how to find a linear model that fits the data well. Before we can do this though, we will need to define what it means for a model to “fit the data well”.\n\n\nMean Squared Error Loss\nOur goal for the loss function is to measure how closely the data fits the prediction made by our model. Intuitively, we should take the difference between the prediction and the true outcome \\(f(x^{(i)})-y^{(i)}\\).\nThe issue with this approach is that \\(f(x^{(i)})-y^{(i)}\\) can be small (negative) even when \\(f(x^{(i)}) \\neq y^{(i)}\\). A natural fix is to take the absolute value \\(|f(x^{(i)}) - y^{(i)}|\\). The benefit of the absolute value is that the loss is \\(0\\) if and only if \\(f(x^{(i)}) = y^{(i)}\\). However, the absolute value function is not differentiable, which is a property we’ll need for optimization. Instead, we use the squared loss:\n\\(\\mathcal{L}(w) = \\frac1{n} \\sum_{i=1}^n (f(x^{(i)}) - y^{(i)})^2\\)\nHere, we use the mean squared error loss, which is the average squared difference between the prediction and the true output over the dataset. Unlike the absolute value function, the squared function is differentiable everywhere. In addition, the squared error penalizes predictions that are far from the true output even more.\n\n\n\nThe plot above compares the squared function to the absolute value function. While both are \\(0\\) if and only if their input is \\(0\\), the squared function is differentiable everywhere and penalizes large errors more.\n\n\nExact Optimization\nWe now have our model and loss function: the linear model and mean squared error loss. The question becomes how to update the weights of the model to minimize the loss. In particular, we want to find \\(w\\) that minimizes \\(\\mathcal{L}(w)\\). While the language we’re using is new, the problem is not. We’ve actually been studying how to do this since pre-calculus!\nThe squared loss is convex (a bowl facing up versus the downward facing cave of concave); see the plot above for a ‘proof’ by picture. In this case, we know there is only one minimum. Not only that but we can find the minimum by setting the derivative to \\(0\\)!\nAs such, our game plan is to set \\(\\frac{\\partial \\mathcal{L}}{\\partial w}\\) to \\(0\\) and solve for \\(w\\). Recall that \\(f(x) = wx\\). We will use the linearity of the derivative, the chain rule, and the power rule to compute the derivative of \\(\\mathcal{L}\\) with respect to \\(w\\):\n\\[\n\\frac{\\partial}{\\partial w}[\\mathcal{L}(w)]\n= \\frac1{n} \\sum_{i=1}^n \\frac{\\partial}{\\partial w} [(f(x^{(i)}) - y^{(i)})^2]\n= \\frac1{n} \\sum_{i=1}^n 2(f(x^{(i)}) - y^{(i)}) \\frac{\\partial}{\\partial w} [(f(x^{(i)}) - y^{(i)})]\n= \\frac1{n} \\sum_{i=1}^n 2(w x^{(i)} - y^{(i)}) x^{(i)}.\n\\]\nSetting the derivative to \\(0\\) and solving for \\(w\\), we get \\(\\frac2{n} \\sum_{i=1}^n w (x^{(i)})^2 = \\frac2{n} \\sum_{i=1}^n y^{(i)} x^{(i)}\\) and so \\[\nw = \\frac{\\sum_{i=1}^n y^{(i)}}{\\sum_{i=1}^n (x^{(i)})^2}.\n\\]\nThis is the exact solution to the univariate linear regression problem! We can now use this formula to find the best linear model for our data. But we’re not done with linear regression yet. We assumed that the input was one-dimensional; however, we often have high-dimensional data."
  },
  {
    "objectID": "notes/01_regression.html#multivariate-linear-regression",
    "href": "notes/01_regression.html#multivariate-linear-regression",
    "title": "Linear Regression and Mean Squared Error",
    "section": "Multivariate Linear Regression",
    "text": "Multivariate Linear Regression\nConsider the more general setting where the input is \\(d\\)-dimensional. As before, we observe \\(n\\) training observations \\((\\mathbf{x}^{(1)}, y^{(1)}), \\ldots, (\\mathbf{x}^{(n)}, y^{(n)})\\) but now \\(\\mathbf{x}^{(i)} \\in \\mathbb{R}^d\\). We will generalize the ideas from univariate linear regression to the multivariate setting.\n\nLinear Model\nInstead of using a single weight \\(w \\in \\mathbb{R}\\), we will use \\(d\\) weights \\(\\mathbf{w} \\in \\mathbb{R}^d\\). Then the model is given by \\(f(x) = \\mathbf{w} \\cdot \\mathbf{x}\\).\nInstead of using a line to fit the data, we use a hyperplane. While visualizing the model is difficult in high dimensions, we can still see the model when \\(d=2\\).\n\n\n\nIn the plot above, we have \\(n=10\\) data points in 3 dimensions. There is one linear model \\(f(\\mathbf{x}) = \\begin{bmatrix} 2 \\\\ \\frac12 \\end{bmatrix} \\cdot \\mathbf{x}\\) that closely approximates the data and another linear model \\(f(\\mathbf{x}) = \\begin{bmatrix} \\frac12 \\\\ 0 \\end{bmatrix} \\cdot \\mathbf{x}\\) that does not approximate the data.\n\n\nMean Squared Error\nSince the output of \\(f\\) is still a single real number, we do not have to change the loss function. However, we can use our linear algebra notation to write the mean squared error in an elegant way.\nLet \\(\\mathbf{X} \\in \\mathbb{R}^{n \\times d}\\) be the data matrix where the \\(i\\)th row is \\((\\mathbf{x}^{(i)})^\\top\\). Similarly, let \\(\\mathbf{y} \\in \\mathbf{R}^n\\) be the target vector where the \\(i\\)th entry is \\(y^{(i)}\\). We can write the mean squared error loss as \\[\n\\mathcal{L}(\\mathbf{w}) = \\frac1{n} \\| \\mathbf{X w - y} \\|_2^2.\n\\]\n\n\nExact Optimization\nJust like computing the derivative and setting it to \\(0\\), we can compute the gradient and set it to the zero vector \\(\\mathbf{0} \\in \\mathbb{R}^d\\). In mathematical notation, we will set \\(\\nabla_\\mathbf{w} \\mathcal{L}(\\mathbf{w}) = \\mathbf{0}\\) and solve for \\(\\mathbf{w}\\).\nWe will leave this as an exercise on the homework. The final solution is that \\(\\mathbf{w} = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{y}\\).\nThis is the exact solution to the multivariate linear regression problem!"
  },
  {
    "objectID": "notes/01_regression.html#conclusion",
    "href": "notes/01_regression.html#conclusion",
    "title": "Linear Regression and Mean Squared Error",
    "section": "Conclusion",
    "text": "Conclusion\nWe’ve learned about the basic building blocks of machine learning and used them to understand linear regression. We’ve seen how to define the model, loss, and optimizer. We’ve also seen how to solve the linear regression problem in both the univariate and multivariate settings.\nThere are two big questions that our discussion of linear regression raises:\n\nWhat if the data does not have a linear relationship?\nWhat happens when our model is too complex to solve for the exact solution?\n\nWe will answer these questions in future lectures as we explore logistic regression, neural networks, and gradient descent."
  },
  {
    "objectID": "notes/01_Regression.html",
    "href": "notes/01_Regression.html",
    "title": "Linear Regression and Mean Squared Error",
    "section": "",
    "text": "When I first heard about machine learning, I imagined a computer that was rewarded every time it gave the right answer. Maybe there were electric carrots and sticks that no one had bothered to tell me about? While I now know as little as I did then about computer hardware, I have learned that machine learning is fundamentally a mathematical process.\nLuckily, we’ve been learning about the very mathematical ideas that make machine learning work for years! We’ll review the basics of these concepts and then jump in to linear regression, arguably the foundation of neural networks.\n\n\nImagine a function \\(\\mathcal{L}: \\mathbb{R} \\to \\mathbb{R}\\). (Instead of the usual \\(f\\), we’ll use \\(\\mathcal{L}\\) for reasons that will soon become clear.) The mapping notation means that \\(\\mathcal{L}\\) takes a single real number as input and outputs a single real number. In general, mathematicians tell us to be careful about whether we can differentiate a function. But, we’re computer scientists so we’ll risk it for the biscuit.\nLet \\(z \\in \\mathbb{R}\\) be the input to \\(\\mathcal{L}\\). The derivative of \\(\\mathcal{L}\\) with respect to its input \\(z\\) is mathematically denoted by \\(\\frac{\\partial}{\\partial z}[\\mathcal{L}(z)]\\).\nFormally, the derivative is defined as \\[\n\\frac{\\partial}{\\partial z}[\\mathcal{L}(z)]\n= \\lim_{h \\to 0} \\frac{\\mathcal{L}(z + h) - \\mathcal{L}(z)}{h}.\n\\] If we were to plot \\(\\mathcal{L}\\), the derivative at a point \\(z\\) would be the slope of the tangent line to the curve at that point.\nHere are several examples of functions and their derivatives that you might remember from calculus.\n\n\n\nFunction: \\(\\mathcal{L}(z)\\) \n\n\nDerivative: \\(\\frac{\\partial}{\\partial z}[\\mathcal{L}(z)]\\)\n\n\n\n\n\\[z^2\\]\n\n\n\\[2z\\]\n\n\n\n\n\\[z^a\\]\n\n\n\\[a z^{a-1}\\]\n\n\n\n\n\\[az + b\\]\n\n\n\\[a\\]\n\n\n\n\n\\[\\ln(z)\\]\n\n\n\\[\\frac{1}{z}\\]\n\n\n\n\n\\[e^z\\]\n\n\n\\[e^z\\]\n\n\n\n\n\n\nWhile working with a simple basic function is easy, we’re not always so lucky. Modern machine learning chains many many complicated functions together. Fortunately, we will think of these operations modularly.\nLet \\(g: \\mathbb{R} \\to \\mathbb{R}\\) be another function. Consider the composite function \\(g(\\mathcal{L}(z))\\).\nBy the chain rule, the derivative of \\(g(\\mathcal{L}(z))\\) with respect to \\(z\\) is \\[\n\\frac{\\partial }{\\partial z}[g(\\mathcal{L}(z))]\n= \\frac{\\partial g}{\\partial z}(\\mathcal{L}(z))\n\\frac{\\partial}{\\partial z}[\\mathcal{L}(z)].\n\\]\nOften, we will also multiply functions together. The product rule tells us that \\[\n\\frac{\\partial }{\\partial z}[g(z) \\mathcal{L}(z)]\n= g(z) \\frac{\\partial}{\\partial z}[\\mathcal{L}(z)]\n+ \\mathcal{L}(z) \\frac{\\partial}{\\partial z}[g(z)].\n\\]\n\n\n\nIn machine learning, we process high-dimensional data so we are interested in functions with multivariate input. Consider \\(\\mathcal{L}: \\mathbb{R}^d \\to \\mathbb{R}\\). The output of the function is still a real number but the input consists of \\(d\\) real numbers. We will use the vector \\(\\mathbf{z} \\in \\mathbb{R}^d\\) to represent all \\(d\\) inputs \\(z_1, \\ldots, z_d\\).\nInstead of the derivative, we will talk use the partial derivative. The partial derivative with respect to \\(z_i\\) is denoted by \\(\\frac{\\partial}{\\partial z_i}[\\mathcal{L}(\\mathbf{z})]\\). In effect, the partial derivative tells us how \\(\\mathcal{L}\\) changes when we change \\(z_i\\) while keeping all other inputs fixed.\nThe gradient stores all the partial derivatives in a vector. The \\(i\\)th entry of this vector is given by the partial derivative of \\(\\mathcal{L}\\) with respect to \\(z_i\\). In mathematical notation, \\[\n\\nabla_\\mathbf{z} \\mathcal{L} = \\left[\\begin{smallmatrix} \\frac{\\partial}{\\partial z_1}[\\mathcal{L}(\\mathbf{z})] \\\\ \\vdots \\\\ \\frac{\\partial}{\\partial z_d}[\\mathcal{L}(\\mathbf{z})] \\\\ \\end{smallmatrix}\\right]\n\\]\nJust like the derivative in one dimension, the gradient contains information about the slope of \\(\\mathcal{L}\\) with respect to each of the \\(d\\) dimensions in its input.\n\n\n\nVector and matrix multiplication lives at the heart of deep learning. In fact, deep learning really started to take off when researchers realized that the Graphical Processing Unit (GPU) could be used to perform gradient updates in addition to the matrix multiplication it was designed to do for gaming.\nConsider two vectors \\(\\mathbf{u} \\in \\mathbb{R}^d\\) and \\(\\mathbf{v} \\in \\mathbb{R}^d\\). We will use \\(\\mathbf{u} \\cdot \\mathbf{v} = \\sum_{i=1}^d u_i v_i\\) to denote the inner product of \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\). The \\(\\mathcal{\\ell}_2\\)-norm of \\(v\\) is given by \\(\\|\\mathbf{v}\\|_2 = \\sqrt{\\mathbf{u} \\cdot \\mathbf{u}}\\).\nConsider two matrices: \\(\\mathbf{A} \\in \\mathbb{R}^{d \\times m}\\) and \\(\\mathbf{B} \\in \\mathbb{R}^{m \\times n}\\) where \\(d \\neq n\\). We can only multiply two matrices when their inner dimension agrees. Because the number of columns in \\(\\mathbf{A}\\) is the same as the number of rows in \\(\\mathbf{B}\\), we can compute \\(\\mathbf{AB}\\). However, because the number of columns in \\(\\mathbf{B}\\) is not the same as the number of rows in \\(\\mathbf{A}\\), the product \\(\\mathbf{BA}\\) is not defined.\nWhen we can multiply two matrices, the \\((i,j)\\) entry in \\(\\mathbf{AB}\\) is given by the inner product between the \\(i\\)th row of \\(\\mathbf{A}\\) and the \\(j\\)th column of \\(\\mathbf{B}\\). The resulting dimensions of the matrix product will be the number of rows in \\(\\mathbf{A}\\) by the number of columns in \\(\\mathbf{B}\\).\n\n\n\nIf we have a scalar equation \\(ax = b\\), we can simply solve for \\(x\\) by dividing both sides by \\(a\\). In effect, we are applying the inverse of \\(a\\) to \\(a\\) i.e., \\(\\frac1{a} a =1\\). The same principle applies to matrices. The \\(n \\times n\\) identity matrix generalizes the scalar identity \\(1\\). This identity matrix is denoted by \\(\\mathbf{I}_{n \\times n} \\in \\mathbb{R}^{n \\times n}\\): the on-diagonal entries \\((i,i)\\) are 1 while the off-diagonal entries \\((i,j)\\) for \\(i\\neq j\\) are 0.\nConsider the matrix equation \\(\\mathbf{Ax} = \\mathbf{b}\\) where \\(\\mathbf{A} \\in \\mathbb{R}^{d \\times d}\\), \\(\\mathbf{x} \\in \\mathbb{R}^d\\), and \\(\\mathbf{b} \\in \\mathbb{R}^d\\). (Notice that the inner dimensions of \\(\\mathbf{A}\\) and \\(\\mathbf{x}\\) agree so their multiplication is well-defined, and the resulting vector is the same dimension as \\(\\mathbf{b}\\).)\nIf we want to solve for \\(\\mathbf{x}\\), we can use the matrix inverse. For a matrix \\(\\mathbf{A}\\), we use \\(\\mathbf{A}^{-1}\\) to denote its inverse. The inverse is defined so that \\(\\mathbf{A}^{-1} \\mathbf{A} = \\mathbf{I}_{n \\times n}\\) where \\(\\mathbf{I}_{n \\times n}\\) is the identity matrix. Then, we can solve for \\(\\mathbf{x}\\) by multiplying both sides of the equation by \\(\\mathbf{A}^{-1}\\). \\[\n\\mathbf{A}^{-1} \\mathbf{Ax} = \\mathbf{A}^{-1} \\mathbf{b}\n\\] Since \\(\\mathbf{A}^{-1} \\mathbf{A} = \\mathbf{I}_{n \\times n}\\), we have that \\(\\mathbf{I}_{n \\times n} \\mathbf{x} = \\mathbf{x} = \\mathbf{A}^{-1} \\mathbf{b}\\)."
  },
  {
    "objectID": "notes/01_Regression.html#math-review",
    "href": "notes/01_Regression.html#math-review",
    "title": "Linear Regression and Mean Squared Error",
    "section": "",
    "text": "When I first heard about machine learning, I imagined a computer that was rewarded every time it gave the right answer. Maybe there were electric carrots and sticks that no one had bothered to tell me about? While I now know as little as I did then about computer hardware, I have learned that machine learning is fundamentally a mathematical process.\nLuckily, we’ve been learning about the very mathematical ideas that make machine learning work for years! We’ll review the basics of these concepts and then jump in to linear regression, arguably the foundation of neural networks.\n\n\nImagine a function \\(\\mathcal{L}: \\mathbb{R} \\to \\mathbb{R}\\). (Instead of the usual \\(f\\), we’ll use \\(\\mathcal{L}\\) for reasons that will soon become clear.) The mapping notation means that \\(\\mathcal{L}\\) takes a single real number as input and outputs a single real number. In general, mathematicians tell us to be careful about whether we can differentiate a function. But, we’re computer scientists so we’ll risk it for the biscuit.\nLet \\(z \\in \\mathbb{R}\\) be the input to \\(\\mathcal{L}\\). The derivative of \\(\\mathcal{L}\\) with respect to its input \\(z\\) is mathematically denoted by \\(\\frac{\\partial}{\\partial z}[\\mathcal{L}(z)]\\).\nFormally, the derivative is defined as \\[\n\\frac{\\partial}{\\partial z}[\\mathcal{L}(z)]\n= \\lim_{h \\to 0} \\frac{\\mathcal{L}(z + h) - \\mathcal{L}(z)}{h}.\n\\] If we were to plot \\(\\mathcal{L}\\), the derivative at a point \\(z\\) would be the slope of the tangent line to the curve at that point.\nHere are several examples of functions and their derivatives that you might remember from calculus.\n\n\n\nFunction: \\(\\mathcal{L}(z)\\) \n\n\nDerivative: \\(\\frac{\\partial}{\\partial z}[\\mathcal{L}(z)]\\)\n\n\n\n\n\\[z^2\\]\n\n\n\\[2z\\]\n\n\n\n\n\\[z^a\\]\n\n\n\\[a z^{a-1}\\]\n\n\n\n\n\\[az + b\\]\n\n\n\\[a\\]\n\n\n\n\n\\[\\ln(z)\\]\n\n\n\\[\\frac{1}{z}\\]\n\n\n\n\n\\[e^z\\]\n\n\n\\[e^z\\]\n\n\n\n\n\n\nWhile working with a simple basic function is easy, we’re not always so lucky. Modern machine learning chains many many complicated functions together. Fortunately, we will think of these operations modularly.\nLet \\(g: \\mathbb{R} \\to \\mathbb{R}\\) be another function. Consider the composite function \\(g(\\mathcal{L}(z))\\).\nBy the chain rule, the derivative of \\(g(\\mathcal{L}(z))\\) with respect to \\(z\\) is \\[\n\\frac{\\partial }{\\partial z}[g(\\mathcal{L}(z))]\n= \\frac{\\partial g}{\\partial z}(\\mathcal{L}(z))\n\\frac{\\partial}{\\partial z}[\\mathcal{L}(z)].\n\\]\nOften, we will also multiply functions together. The product rule tells us that \\[\n\\frac{\\partial }{\\partial z}[g(z) \\mathcal{L}(z)]\n= g(z) \\frac{\\partial}{\\partial z}[\\mathcal{L}(z)]\n+ \\mathcal{L}(z) \\frac{\\partial}{\\partial z}[g(z)].\n\\]\n\n\n\nIn machine learning, we process high-dimensional data so we are interested in functions with multivariate input. Consider \\(\\mathcal{L}: \\mathbb{R}^d \\to \\mathbb{R}\\). The output of the function is still a real number but the input consists of \\(d\\) real numbers. We will use the vector \\(\\mathbf{z} \\in \\mathbb{R}^d\\) to represent all \\(d\\) inputs \\(z_1, \\ldots, z_d\\).\nInstead of the derivative, we will talk use the partial derivative. The partial derivative with respect to \\(z_i\\) is denoted by \\(\\frac{\\partial}{\\partial z_i}[\\mathcal{L}(\\mathbf{z})]\\). In effect, the partial derivative tells us how \\(\\mathcal{L}\\) changes when we change \\(z_i\\) while keeping all other inputs fixed.\nThe gradient stores all the partial derivatives in a vector. The \\(i\\)th entry of this vector is given by the partial derivative of \\(\\mathcal{L}\\) with respect to \\(z_i\\). In mathematical notation, \\[\n\\nabla_\\mathbf{z} \\mathcal{L} = \\left[\\begin{smallmatrix} \\frac{\\partial}{\\partial z_1}[\\mathcal{L}(\\mathbf{z})] \\\\ \\vdots \\\\ \\frac{\\partial}{\\partial z_d}[\\mathcal{L}(\\mathbf{z})] \\\\ \\end{smallmatrix}\\right]\n\\]\nJust like the derivative in one dimension, the gradient contains information about the slope of \\(\\mathcal{L}\\) with respect to each of the \\(d\\) dimensions in its input.\n\n\n\nVector and matrix multiplication lives at the heart of deep learning. In fact, deep learning really started to take off when researchers realized that the Graphical Processing Unit (GPU) could be used to perform gradient updates in addition to the matrix multiplication it was designed to do for gaming.\nConsider two vectors \\(\\mathbf{u} \\in \\mathbb{R}^d\\) and \\(\\mathbf{v} \\in \\mathbb{R}^d\\). We will use \\(\\mathbf{u} \\cdot \\mathbf{v} = \\sum_{i=1}^d u_i v_i\\) to denote the inner product of \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\). The \\(\\mathcal{\\ell}_2\\)-norm of \\(v\\) is given by \\(\\|\\mathbf{v}\\|_2 = \\sqrt{\\mathbf{u} \\cdot \\mathbf{u}}\\).\nConsider two matrices: \\(\\mathbf{A} \\in \\mathbb{R}^{d \\times m}\\) and \\(\\mathbf{B} \\in \\mathbb{R}^{m \\times n}\\) where \\(d \\neq n\\). We can only multiply two matrices when their inner dimension agrees. Because the number of columns in \\(\\mathbf{A}\\) is the same as the number of rows in \\(\\mathbf{B}\\), we can compute \\(\\mathbf{AB}\\). However, because the number of columns in \\(\\mathbf{B}\\) is not the same as the number of rows in \\(\\mathbf{A}\\), the product \\(\\mathbf{BA}\\) is not defined.\nWhen we can multiply two matrices, the \\((i,j)\\) entry in \\(\\mathbf{AB}\\) is given by the inner product between the \\(i\\)th row of \\(\\mathbf{A}\\) and the \\(j\\)th column of \\(\\mathbf{B}\\). The resulting dimensions of the matrix product will be the number of rows in \\(\\mathbf{A}\\) by the number of columns in \\(\\mathbf{B}\\).\n\n\n\nIf we have a scalar equation \\(ax = b\\), we can simply solve for \\(x\\) by dividing both sides by \\(a\\). In effect, we are applying the inverse of \\(a\\) to \\(a\\) i.e., \\(\\frac1{a} a =1\\). The same principle applies to matrices. The \\(n \\times n\\) identity matrix generalizes the scalar identity \\(1\\). This identity matrix is denoted by \\(\\mathbf{I}_{n \\times n} \\in \\mathbb{R}^{n \\times n}\\): the on-diagonal entries \\((i,i)\\) are 1 while the off-diagonal entries \\((i,j)\\) for \\(i\\neq j\\) are 0.\nConsider the matrix equation \\(\\mathbf{Ax} = \\mathbf{b}\\) where \\(\\mathbf{A} \\in \\mathbb{R}^{d \\times d}\\), \\(\\mathbf{x} \\in \\mathbb{R}^d\\), and \\(\\mathbf{b} \\in \\mathbb{R}^d\\). (Notice that the inner dimensions of \\(\\mathbf{A}\\) and \\(\\mathbf{x}\\) agree so their multiplication is well-defined, and the resulting vector is the same dimension as \\(\\mathbf{b}\\).)\nIf we want to solve for \\(\\mathbf{x}\\), we can use the matrix inverse. For a matrix \\(\\mathbf{A}\\), we use \\(\\mathbf{A}^{-1}\\) to denote its inverse. The inverse is defined so that \\(\\mathbf{A}^{-1} \\mathbf{A} = \\mathbf{I}_{n \\times n}\\) where \\(\\mathbf{I}_{n \\times n}\\) is the identity matrix. Then, we can solve for \\(\\mathbf{x}\\) by multiplying both sides of the equation by \\(\\mathbf{A}^{-1}\\). \\[\n\\mathbf{A}^{-1} \\mathbf{Ax} = \\mathbf{A}^{-1} \\mathbf{b}\n\\] Since \\(\\mathbf{A}^{-1} \\mathbf{A} = \\mathbf{I}_{n \\times n}\\), we have that \\(\\mathbf{I}_{n \\times n} \\mathbf{x} = \\mathbf{x} = \\mathbf{A}^{-1} \\mathbf{b}\\)."
  },
  {
    "objectID": "notes/01_Regression.html#univariate-linear-regression",
    "href": "notes/01_Regression.html#univariate-linear-regression",
    "title": "Linear Regression and Mean Squared Error",
    "section": "Univariate Linear Regression",
    "text": "Univariate Linear Regression\nNow that we’ve reviewed the basic building blocks of machine learning, we can dive into linear regression. Linear regression is a simple and powerful tool that we will use to understand the basics of deep learning.\n\nGoal\nWe will begin our study of deep learning in the supervised setting. In this setting, we are given labelled data with input features and an outcome. Formally, we will have \\(n\\) labelled observations \\((x^{(1)}, y^{(1)}), \\ldots, (x^{(n)}, y^{(n)})\\). In general, we will have \\(y \\in \\mathbb{R}\\). For simplicity, we will assume for now that \\(x \\in \\mathbb{R}\\).\nOur goal is to process the data and learn a function that approximates the outcomes. In mathematical notation, we want to learn a function \\(f: \\mathbb{R} \\to \\mathbb{R}\\) so that \\(f(x^{(i)}) \\approx y^{(i)}\\) for the \\(n\\) labelled observations \\(i \\in \\{1,\\ldots,n\\}\\).\nBefore we dive into the specific way we will accomplish this with linear regression, let’s discuss the general deep learning framework. This three-step framework gives a flexible scaffolding that we will use to understand almost every topic in this course.\nThe three-step framework includes:\n• The Model: The function that we’ll use to process the input and produce a corresponding output.\n• The Loss: The function that measures the quality of the outputs from our model. (Without loss of generality, we will assume that lower is better.)\n• The Optimizer: The method of updating the model to improve the loss.\nWith these general concepts in mind, we’ll explore linear regression.\n\n\nLinear Model\nAs its name suggests, linear regression uses a linear model to process the input into an approximation of the output. Let \\(w \\in \\mathbb{R}\\) be a weight parameter. The linear model (for one-dimensional inputs) is given by \\(f(x) = wx\\).\nUnlike many deep learning models, we can visualize the linear model since it is given by a line. In the plot, we have the \\(n=10\\) data points plotted in 2 dimensions. There is one linear model \\(f(x) = 2x\\) that closely approximates the data and another linear model \\(f(x)=\\frac12 x\\) that does not approximate the data.\n\n\n\nOur goal is to learn how to find a linear model that fits the data well. Before we can do this though, we will need to define what it means for a model to “fit the data well”.\n\n\nMean Squared Error Loss\nOur goal for the loss function is to measure how closely the data fits the prediction made by our model. Intuitively, we should take the difference between the prediction and the true outcome \\(f(x^{(i)})-y^{(i)}\\).\nThe issue with this approach is that \\(f(x^{(i)})-y^{(i)}\\) can be small (negative) even when \\(f(x^{(i)}) \\neq y^{(i)}\\). A natural fix is to take the absolute value \\(|f(x^{(i)}) - y^{(i)}|\\). The benefit of the absolute value is that the loss is \\(0\\) if and only if \\(f(x^{(i)}) = y^{(i)}\\). However, the absolute value function is not differentiable, which is a property we’ll need for optimization. Instead, we use the squared loss:\n\\(\\mathcal{L}(w) = \\frac1{n} \\sum_{i=1}^n (f(x^{(i)}) - y^{(i)})^2\\)\nHere, we use the mean squared error loss, which is the average squared difference between the prediction and the true output over the dataset. Unlike the absolute value function, the squared function is differentiable everywhere. In addition, the squared error penalizes predictions that are far from the true output even more.\n\n\n\nThe plot above compares the squared function to the absolute value function. While both are \\(0\\) if and only if their input is \\(0\\), the squared function is differentiable everywhere and penalizes large errors more.\n\n\nExact Optimization\nWe now have our model and loss function: the linear model and mean squared error loss. The question becomes how to update the weights of the model to minimize the loss. In particular, we want to find \\(w\\) that minimizes \\(\\mathcal{L}(w)\\). While the language we’re using is new, the problem is not. We’ve actually been studying how to do this since pre-calculus!\nThe squared loss is convex (a bowl facing up versus the downward facing cave of concave); see the plot above for a ‘proof’ by picture. In this case, we know there is only one minimum. Not only that but we can find the minimum by setting the derivative to \\(0\\)!\nAs such, our game plan is to set \\(\\frac{\\partial \\mathcal{L}}{\\partial w}\\) to \\(0\\) and solve for \\(w\\). Recall that \\(f(x) = wx\\). We will use the linearity of the derivative, the chain rule, and the power rule to compute the derivative of \\(\\mathcal{L}\\) with respect to \\(w\\):\n\\[\n\\frac{\\partial}{\\partial w}[\\mathcal{L}(w)]\n= \\frac1{n} \\sum_{i=1}^n \\frac{\\partial}{\\partial w} [(f(x^{(i)}) - y^{(i)})^2]\n= \\frac1{n} \\sum_{i=1}^n 2(f(x^{(i)}) - y^{(i)}) \\frac{\\partial}{\\partial w} [(f(x^{(i)}) - y^{(i)})]\n= \\frac1{n} \\sum_{i=1}^n 2(w x^{(i)} - y^{(i)}) x^{(i)}.\n\\]\nSetting the derivative to \\(0\\) and solving for \\(w\\), we get \\(\\frac2{n} \\sum_{i=1}^n w (x^{(i)})^2 = \\frac2{n} \\sum_{i=1}^n y^{(i)} x^{(i)}\\) and so \\[\nw = \\frac{\\sum_{i=1}^n y^{(i)}}{\\sum_{i=1}^n (x^{(i)})^2}.\n\\]\nThis is the exact solution to the univariate linear regression problem! We can now use this formula to find the best linear model for our data. But we’re not done with linear regression yet. We assumed that the input was one-dimensional; however, we often have high-dimensional data."
  },
  {
    "objectID": "notes/01_Regression.html#multivariate-linear-regression",
    "href": "notes/01_Regression.html#multivariate-linear-regression",
    "title": "Linear Regression and Mean Squared Error",
    "section": "Multivariate Linear Regression",
    "text": "Multivariate Linear Regression\nConsider the more general setting where the input is \\(d\\)-dimensional. As before, we observe \\(n\\) training observations \\((\\mathbf{x}^{(1)}, y^{(1)}), \\ldots, (\\mathbf{x}^{(n)}, y^{(n)})\\) but now \\(\\mathbf{x}^{(i)} \\in \\mathbb{R}^d\\). We will generalize the ideas from univariate linear regression to the multivariate setting.\n\nLinear Model\nInstead of using a single weight \\(w \\in \\mathbb{R}\\), we will use \\(d\\) weights \\(\\mathbf{w} \\in \\mathbb{R}^d\\). Then the model is given by \\(f(x) = \\mathbf{w} \\cdot \\mathbf{x}\\).\nInstead of using a line to fit the data, we use a hyperplane. While visualizing the model is difficult in high dimensions, we can still see the model when \\(d=2\\).\n\n\n\nIn the plot above, we have \\(n=10\\) data points in 3 dimensions. There is one linear model \\(f(\\mathbf{x}) = \\begin{bmatrix} 2 \\\\ \\frac12 \\end{bmatrix} \\cdot \\mathbf{x}\\) that closely approximates the data and another linear model \\(f(\\mathbf{x}) = \\begin{bmatrix} \\frac12 \\\\ 0 \\end{bmatrix} \\cdot \\mathbf{x}\\) that does not approximate the data.\n\n\nMean Squared Error\nSince the output of \\(f\\) is still a single real number, we do not have to change the loss function. However, we can use our linear algebra notation to write the mean squared error in an elegant way.\nLet \\(\\mathbf{X} \\in \\mathbb{R}^{n \\times d}\\) be the data matrix where the \\(i\\)th row is \\((\\mathbf{x}^{(i)})^\\top\\). Similarly, let \\(\\mathbf{y} \\in \\mathbf{R}^n\\) be the target vector where the \\(i\\)th entry is \\(y^{(i)}\\). We can write the mean squared error loss as \\[\n\\mathcal{L}(\\mathbf{w}) = \\frac1{n} \\| \\mathbf{X w - y} \\|_2^2.\n\\]\n\n\nExact Optimization\nJust like computing the derivative and setting it to \\(0\\), we can compute the gradient and set it to the zero vector \\(\\mathbf{0} \\in \\mathbb{R}^d\\). In mathematical notation, we will set \\(\\nabla_\\mathbf{w} \\mathcal{L}(\\mathbf{w}) = \\mathbf{0}\\) and solve for \\(\\mathbf{w}\\).\nWe will leave this as an exercise on the homework. The final solution is that \\(\\mathbf{w} = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{y}\\).\nThis is the exact solution to the multivariate linear regression problem!"
  },
  {
    "objectID": "notes/01_Regression.html#conclusion",
    "href": "notes/01_Regression.html#conclusion",
    "title": "Linear Regression and Mean Squared Error",
    "section": "Conclusion",
    "text": "Conclusion\nWe’ve learned about the basic building blocks of machine learning and used them to understand linear regression. We’ve seen how to define the model, loss, and optimizer. We’ve also seen how to solve the linear regression problem in both the univariate and multivariate settings.\nThere are two big questions that our discussion of linear regression raises:\n\nWhat if the data does not have a linear relationship?\nWhat happens when our model is too complex to solve for the exact solution?\n\nWe will answer these questions in future lectures as we explore logistic regression, neural networks, and gradient descent."
  },
  {
    "objectID": "notes/08_Watermarking.html",
    "href": "notes/08_Watermarking.html",
    "title": "Watermarking LLMs",
    "section": "",
    "text": "Written by Lenox Herman"
  },
  {
    "objectID": "notes/08_Watermarking.html#motivation",
    "href": "notes/08_Watermarking.html#motivation",
    "title": "Watermarking LLMs",
    "section": "Motivation",
    "text": "Motivation\nAs LLMs (large language models) continue to be used and grow in modern times, the need for delineating where text originates becomes more apparent. For instance, a teacher might want to determine if a student has used an LLM to complete an assignment. On the flip side, organizations like OpenAI might need a way to verify whether a given output can genuinely be attributed to their models. In order to do this, LLMs use watermarking to determine whether a block of text was human or machine-generated.\nLet’s look at the example, “Turmeric lemon cookies are…” In order to calculate the next word, LLMs embed each word to create a probability distribution to determine the next word."
  },
  {
    "objectID": "notes/08_Watermarking.html#redgreen-listing",
    "href": "notes/08_Watermarking.html#redgreen-listing",
    "title": "Watermarking LLMs",
    "section": "Red/Green Listing",
    "text": "Red/Green Listing\nThis is a watermarking method where words are predetermined as “Red” and “Green” words. Here, a small constant is added to all green words before the softmax is used to calculate the probability distribution. The result is that green words are slightly more likely to be selected when generating text.\nTo detect Red/Green watermarking on LLM-generated text, you would run a P-test. Consider the example:\n“Watermarking is a very fun topic to cover especially when Teal is teaching it.”\nHere we assume that the probability of green words appearing in natural text is 50/50, or 1/2.\nTo detect Red/Green watermarking in LLM-generated text, a statistical hypothesis test can be applied. Suppose the probability of selecting a green word in natural text is 50% (i.e., \\(P(\\text{Green}) = 0.5\\)). We can analyze the observed frequency of green words in \\(m\\) trials (words in the text) to determine if the text is likely watermarked.\nThe probability of observing exactly \\(K\\) green words in \\(m\\) trials follows a binomial distribution:\nP (k success in m trials) = [ (Pr())^K (1 - Pr())^{m-K}, ]\nwhere \\(m\\) \\(K\\) is the binomial coefficient.\nBy comparing the observed frequency of green words to the expected frequency under the null hypothesis (50/50 distribution), we can infer whether the text is watermarked.\nThe full summation form of this probability is: P (k success in m trials) = [ _{l=k}^m p^l (1 - p)^{m-l} ]\nwhere \\(p\\) is the probability of success (e.g., selecting a green word).\nModifying the probability distribution in watermarking slightly affects the output quality. Selecting a word that is not the highest probability increases the risk of hallucinations since LLMs are auto-generative and build text sequentially.\nAdditionally, if someone gains access to the model, they could potentially reverse-engineer the watermarking process by distinguishing red words from green words. For example, the word “delve” could be identified as a green word generated by ChatGPT, providing evidence of watermarking being used."
  },
  {
    "objectID": "notes/08_Watermarking.html#distortion-free-watermarking",
    "href": "notes/08_Watermarking.html#distortion-free-watermarking",
    "title": "Watermarking LLMs",
    "section": "Distortion-Free Watermarking",
    "text": "Distortion-Free Watermarking\nTo solve the issue of modifying the probability distribution, distortion-free watermarking provides a method to embed a watermark without altering the output probabilities significantly. For example, consider the sentence:\n“Vermont kale in the winter! ___”\nIn this approach:\n\nThe tokens within a designated window, let’s say the last three words—“in”, “the”, “winter” for this example—are converted into numerical values and summed to create a seed.\nThis seed is a multiplier to generate a random yet replicable sampling from the probability distribution of the next word without introducing distortion.\n\nTo detect this watermark, one would:\n\nAccess the prompt or the preceding context (e.g., “Vermont kale in the winter!”).\nExtract the last three tokens, regenerate the seed, and sample from the given probability distribution.\nCompare the generated probability distribution with the observed text.\n\nThis approach ensures the watermark is detectable without modifying the core probability distribution of the language model; however, it is still an unreasonable solution because it requires the model to have access to the prompt as well."
  },
  {
    "objectID": "notes/08_Watermarking.html#exponential-minimum-sampling",
    "href": "notes/08_Watermarking.html#exponential-minimum-sampling",
    "title": "Watermarking LLMs",
    "section": "Exponential Minimum Sampling",
    "text": "Exponential Minimum Sampling\nExponential Minimum Sampling addresses the core challenges of watermarking: it does not require access to the prompt and avoids altering the probability distribution. Here, a pseudo-random vector \\(X\\) is generated using a seed derived from a sliding window of the given text. This \\(X\\) vector has continuous values between 0 and 1, with a length equal to the dimensionality of the vocabulary, \\(|v|\\).\nThe next token is selected based on minimizing the cost function:\n\\(\\text{Cost} = -\\frac{\\log(x_i)}{\\text{len(token\\_ids)}},\\)\nwhere \\(x_i\\) are entries from the \\(X\\) vector. This method ensures that the probability of selecting a specific token \\(i^*\\) corresponds directly to its true likelihood \\(P(i^*)\\) in the original probability distribution.\n\nPseudo-code for Detection:\ncost = 0\nfor token_id in token_ids:\n    seed = calculate_seed(token_id, window)\n    x_vector = generate_x_vector(seed)\n    cost += -log(x_vector[token_id]) / len(token_ids)\nThe probability of selecting the next word can be expressed as:\n\\(\\text{next} = \\arg\\min_i \\frac{-\\log x_i}{p_i},\\) where \\(x \\sim \\text{Uniform}(0, 1)^|v|\\).\nAdditionally we know the probobility of selecting the next word, i, is just P(i) because:\n\\(P\\left(-\\frac{\\log x_i}{p_i} \\geq t\\right) = P\\left(x_i \\leq e^{-p_i t}\\right) = e^{-p_i t}.\\)\nThus, the probability density of \\(-\\frac{\\log x_i}{p_i}\\) is: \\(P\\left(-\\frac{\\log x_i}{p_i} = u\\right) = p_i \\cdot e^{-p_i u}.\\)\nThe cumulative probability of the selected word being \\(\\arg\\min_i \\frac{-\\log x_i}{p_i}\\) is: \\(P\\left(i^* = \\arg\\min_i \\frac{-\\log x_i}{p_i}\\right) =\n\\int_{u=t}^\\infty P\\left(-\\frac{\\log x_i}{p_i} = u\\right)\n\\prod_{j \\neq i} P\\left(-\\frac{\\log x_j}{p_j} &gt; t\\right) \\, du.\\)\nSimplifying further: \\(P\\left(i^* = \\arg\\min_i \\frac{-\\log x_i}{p_i}\\right) =\np_i \\int_{u=t}^\\infty e^{-p_i u} \\prod_{j \\neq i} e^{-p_j u} \\, du.\\)\nExpanding the product term:\n\\(P\\left(i^* = \\arg\\min_i \\frac{-\\log x_i}{p_i}\\right) =\np_i \\int_{u=t}^\\infty e^{-\\left(\\sum_{j} p_j\\right) u} \\, du.\\)\nFinally, solving the integral:\n\\(P\\left(i^* = \\arg\\min_i \\frac{-\\log x_i}{p_i}\\right) =\np_i \\cdot \\left[-\\frac{1}{\\sum_{j} p_j} e^{-\\left(\\sum_{j} p_j\\right) u}\\right]_t^\\infty,\\)\nwhich simplifies to:\n\\(P\\left(i^* = \\arg\\min_i \\frac{-\\log x_i}{p_i}\\right) = p_i.\\)\nThe seed for this process is \\(x\\)."
  }
]