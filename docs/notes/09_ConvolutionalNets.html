<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.55">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Convolutional Neural Networks and Image Embeddings</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../favicon.ico" rel="icon">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<link rel="shortcut icon" href="favicon.ico">
<script id="MathJax-script" async="" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>


<!-- Google tag (gtag.js) -->
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-GZHXTPTRRE"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-GZHXTPTRRE');
</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Winter 2025</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="https://middlebury.instructure.com/courses/16004"> 
<span class="menu-text">Canvas</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://www.gradescope.com/courses/934368"> 
<span class="menu-text">Gradescope</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../syllabus.html"> 
<span class="menu-text">Syllabus</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-prior-years" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Prior Years</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-prior-years">    
        <li>
    <a class="dropdown-item" href="https://www.rtealwitter.com/deeplearning2023/">
 <span class="dropdown-text">Winter 2023</span></a>
  </li>  
    </ul>
  </li>
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#motivation" id="toc-motivation" class="nav-link active" data-scroll-target="#motivation">Motivation</a></li>
  <li><a href="#enter-convolution" id="toc-enter-convolution" class="nav-link" data-scroll-target="#enter-convolution">Enter Convolution…</a></li>
  <li><a href="#convolution-in-2-dimensions" id="toc-convolution-in-2-dimensions" class="nav-link" data-scroll-target="#convolution-in-2-dimensions">Convolution in 2 Dimensions</a></li>
  <li><a href="#kernels-features-and-more" id="toc-kernels-features-and-more" class="nav-link" data-scroll-target="#kernels-features-and-more">Kernels, Features, and More…</a></li>
  <li><a href="#upsampling" id="toc-upsampling" class="nav-link" data-scroll-target="#upsampling">Upsampling</a></li>
  <li><a href="#embeddings" id="toc-embeddings" class="nav-link" data-scroll-target="#embeddings">Embeddings</a>
  <ul class="collapse">
  <li><a href="#autoencoders" id="toc-autoencoders" class="nav-link" data-scroll-target="#autoencoders">Autoencoders</a></li>
  <li><a href="#contrastive-learning" id="toc-contrastive-learning" class="nav-link" data-scroll-target="#contrastive-learning">Contrastive Learning</a></li>
  <li><a href="#variational-autoencoders" id="toc-variational-autoencoders" class="nav-link" data-scroll-target="#variational-autoencoders">Variational Autoencoders</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-page-left" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><strong>Convolutional Neural Networks and Image Embeddings</strong></h1>
</div>



<div class="quarto-title-meta column-page-left">

    
  
    
  </div>
  


</header>


<p><em>Written by Jayda Gilyard and Josef Liem</em></p>
<section id="motivation" class="level3">
<h3 class="anchored" data-anchor-id="motivation">Motivation</h3>
<p>In class, we have already covered some useful kinds of layers that we can include in our model: fully connected layers (FCLs), attention layers, activation layers (Sigmoid, Softmax, ReLU, etc…). Today, we will talk about new kind of layer that can be used called <em>convolution</em>.</p>
<p><em>Why should we care about convolution when we already saw that we can perform classification using a sequence of linear layers and activations?</em></p>
<center>
<img src="images/FullyConnectedLayer.png" width="700">
</center>
<p>If we are interested in working with large input vectors, this will involve massive matrix multiplication with time complexity <span class="math inline">\(\theta(dd')\)</span> where <span class="math inline">\(d\)</span> denotes the size of the input vector, an <span class="math inline">\(d'\)</span> the output vector. In 2D, this is particularly problematic if we wanted to build an image classifier for, say, 1024x1024 images, as we would be performing many large matrix multiplications. Thus, given that a model may consist of many linear layers, the number of trainable parameters in the model becomes far too expensive to handle at scale.</p>
</section>
<section id="enter-convolution" class="level3">
<h3 class="anchored" data-anchor-id="enter-convolution">Enter Convolution…</h3>
<p>Instead, we can take advantage of the notion of <em>shared weights</em> in convolution through a <em>kernel</em> (also called a <em>filter</em>), to be far more efficient. Let us first take a look at the 1-dimensional case for convolution.</p>
<center>
<img src="images/1DConvolution.png" width="700">
</center>
<p>In our convolutional layer, our <em>kernel</em> is composed of a set of learnable weights for which we take the inner product with entries of the input vector, summing over it. We then slide this kernel over by a certain <em>stride</em> to generate the next scalar entry in the output vector.</p>
<p>More formally, where <span class="math inline">\(y\)</span>, is the result of a summation over the inner products of the input <span class="math inline">\(x\)</span> and weights <span class="math inline">\(w\)</span> of the kernel, we can express 1-dimensional convolution as:</p>
<p><span class="math display">\[y[n]=\sum_{i}x[i] ⋅ w[n-i]\]</span></p>
<p>In this new kind of layer, the weights in the kernel are shared with the entirety of the entries of the input, rather than defining a weight for each input/output pair, giving us complexity <span class="math inline">\(\theta(w) + \theta(d'w)\)</span>.</p>
<p>There are several major advantages to using convolution:</p>
<ol type="1">
<li>We can use shared weights rather than having weights for each input/output entry pair.</li>
<li>We can be remarkably more efficient by not performing massive matrix multiplications.</li>
<li>We can take advantage of spatial locality in the input, which can be good when our data consists of sequential information. That is, in almost all images (with the exception of noise perhaps), nearby pixels are more likely to be related to each other than pixels that are far apart.</li>
</ol>
</section>
<section id="convolution-in-2-dimensions" class="level3">
<h3 class="anchored" data-anchor-id="convolution-in-2-dimensions">Convolution in 2 Dimensions</h3>
<p>When we are performing convolution in machine learning, we are usually talking about 2-D convolution on images. Here, rather than a vector, our kernel now takes the form of a <span class="math inline">\(w' * w'\)</span> matrix <span class="math inline">\(W\)</span> which we slide over the <span class="math inline">\(d * d\)</span> input, taking the summation over the inner product of weights and entries, producing a <span class="math inline">\(d' * d'\)</span> output.</p>
<center>
<img src="images/2DConvolution.png" width="700">
</center>
<p>We only need to make some slight adjustments to the previous definition of 1-dimensional discrete convolution to reach 2-dimensions:</p>
<p><span class="math display">\[y[m, n]=\sum_{i}\sum_{j}x[i, j] ⋅ w[i-m, j-n]\]</span></p>
<ul>
<li><span class="math inline">\(x[i, j]\)</span> denotes the input at position <span class="math inline">\(i, j\)</span>.</li>
<li><span class="math inline">\(y[m, n]\)</span> denotes the output at position <span class="math inline">\(m, n\)</span>.</li>
<li><span class="math inline">\(w[i-m, j-n]\)</span> denotes the kernel weights with some shift over the input.</li>
</ul>
<p>Note that the resulting shape of the output of convolution heavily depends on the shape of of <span class="math inline">\(x\)</span>, the shape of <span class="math inline">\(w\)</span>, and thus the number of strides the kernel is able to take across the input. In addition to choosing a different kernel or input shape, there are several notable modifications during convolution that will allow us to control the <span class="math inline">\(d' * d'\)</span> shape of the output:</p>
<ol type="1">
<li><strong><em>Stride</em></strong></li>
</ol>
<center>
<img src="images/StridedConvolution.png" width="700">
</center>
<p>We can set our <em>stride</em> to be some value <span class="math inline">\(s\)</span>, such that our kernel will move by <span class="math inline">\(s\)</span> entries when taking the summation over the inner product of input/kernel entries and weights. Increasingly large strides in convolution reduce the size of the spatial dimensions of the output.</p>
<ol start="2" type="1">
<li><strong><em>Padding</em></strong></li>
</ol>
<center>
<img src="images/PaddedConvolution.png" width="700">
</center>
<p>We can apply padding (usually a bunch of 0s) around our input <span class="math inline">\(x\)</span>. Adding padding will change the number of strides we might take in the given row height/column height, affecting the shape of our output.</p>
<ol start="3" type="1">
<li><strong><em>Dilation</em></strong></li>
</ol>
<center>
<img src="images/DilatedConvolution.png" width="700">
</center>
<p>Dilation is another technique that can be applied to convolutional layers to expand the <em>receptive field</em> of the kernel without increasing the number of parameters.</p>
</section>
<section id="kernels-features-and-more" class="level3">
<h3 class="anchored" data-anchor-id="kernels-features-and-more">Kernels, Features, and More…</h3>
<p><em>What in an image is the kernel (hopefully) picking up on?</em></p>
<p>Ideally, in an image context, convolution is nice because kernels can pick up on features such as types of edges, corners, textures, and shapes when trying to perform classification. If we use sequences of convolutional layers, deeper layers might capture more abstract or complex features, whereas initial layers focus on more fine grained detail. Additionally, in practice, we usually apply several convolutional kernels to pick up on different kinds of features at a given layer, determining the number of <em>output channels</em> that the layer will produce. Many convolutional-based architectures will tend to increase the number of <em>channels</em> while reducing the overall <em>spatial dimensions</em> at subsequent layers.</p>
</section>
<section id="upsampling" class="level3">
<h3 class="anchored" data-anchor-id="upsampling">Upsampling</h3>
<p><em>What about upsampling?</em></p>
<p>As our previous illustrations have implicitly shown, we often talk about convolution as downsampling the original input such that the spatial dimensions gradually decrease at each convolutional layer. However, upsampling is not only possible, but often necessary in models the use an <em>autoencoder</em>-based architecture for image generation/reconstruction (see autoencoders next section). This is called <em>transposed convolution</em> or also <em>deconvolution</em>.</p>
</section>
<section id="embeddings" class="level2">
<h2 class="anchored" data-anchor-id="embeddings">Embeddings</h2>
<section id="autoencoders" class="level3">
<h3 class="anchored" data-anchor-id="autoencoders">Autoencoders</h3>
<center>
<img src="images/Autoencoder.png" width="700">
</center>
<p>For embeddings over images, we can use an <em>autoencoder</em>, which consists of an <em>encoder</em> and a <em>decoder</em>. The encoder applies a set of <em>kernels</em> through several convolutional layers to generate a latent vector <span class="math inline">\(z\)</span> — a compressed representation of our image. Subsequent <em>transposed convolutions</em> in the <em>decoder</em> layers can reconstruct an output image.</p>
<p>For the loss, instead of <em>cross entropy</em>, we can use <em>mean-squared error</em>, performed pixel-wise to look at “distance” between images:</p>
<p><span class="math display">\[L(w)=||g(f(x))-x||^2_2\]</span></p>
</section>
<section id="contrastive-learning" class="level3">
<h3 class="anchored" data-anchor-id="contrastive-learning">Contrastive Learning</h3>
<center>
<img src="images/ContrastiveLearning.png" width="700">
</center>
<p>We have also seen an alternative approach - <em>contrastive learning</em>, that would allow us to perform initial training without the <em>decoder</em>. In this approach, we take positive pairs <span class="math inline">\((x, x^+)\)</span> and negative pairs <span class="math inline">\((x, x^-)\)</span> of images. The goal of contrastive learning is to have positive pairs end up close together in <em>latent space</em> and negative pairs far apart. Equivalently:</p>
<ol type="1">
<li>The inner product of <span class="math inline">\(f(x)^Tf(x^+)\)</span> should be large</li>
<li>The inner product of <span class="math inline">\(f(x)^Tf(x^-)\)</span> should be small</li>
</ol>
<p><em>How do we sample positive and negative pairs of images?</em></p>
<ol type="1">
<li>Someone manually labelled positive and negative pairs of images (supervised)</li>
<li>We perform croppings on a given image and sample croppings of the same larger image are considered positive pairs, and unrelated croppings are negative</li>
</ol>
</section>
<section id="variational-autoencoders" class="level3">
<h3 class="anchored" data-anchor-id="variational-autoencoders">Variational Autoencoders</h3>
<p>While autoencoders are a nice approach, notice that unlike contrastive learning where we try to separate negative pairs, nothing about the autoencoder forces even spacing in the latent space, allowing for potential collisions between instances of differing classes of images.</p>
<center>
<img src="images/AutoencoderCollisions.png" width="400">
</center>
<p>Variational autoencoders (VAEs) solve this problem probabilistically by introducing a probabilistic encoder and decoder. Instead of mapping an input to a single point in the latent space, the encoder maps the input to a distribution over the latent space. The decoder then samples from this distribution to reconstruct the input.</p>
<center>
<img src="images/VariationalAutoencoder.png" width="700">
</center>
<p>The encoder outputs the mean <span class="math inline">\(\mu\)</span> and standard deviation <span class="math inline">\(\sigma\)</span> of the latent distribution, and we sample a latent vector <span class="math inline">\(z\sim\mathcal{N}(\mu, \text{diag}(\sigma^2))\)</span> from this distribution using:</p>
<p><span class="math display">\[z = \mu + \sigma \epsilon\]</span></p>
<p>where <span class="math inline">\(\epsilon\)</span> is sampled from a Gaussian distribution <span class="math inline">\(\epsilon\sim\mathcal{N}(0, I)\)</span>.</p>
<p>The loss function for VAEs consists of two parts:</p>
<ol type="1">
<li>Reconstruction loss: <span class="math inline">\(L_{\text{reconstruction}}=||g(f(x))-x||^2_2\)</span>. This measures how well the decoder reconstructs the input from the latent vector.</li>
<li>KL divergence: <span class="math inline">\(L_{\text{variational}}=\text{dist}(\mathcal{N}(0, I), \mathcal{N}(\mu, \text{diag}(\sigma^2)))\)</span>. This measures how close the learned latent distribution is to the normal distribution.</li>
</ol>
<p>Thus, the reconstruction is both meaningful and the latent space is evenly distributed.</p>



</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>