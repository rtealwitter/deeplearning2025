<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.47">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Linear Regression and Mean Squared Error</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../favicon.ico" rel="icon">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<link rel="shortcut icon" href="favicon.ico">
<script id="MathJax-script" async="" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>


<!-- Google tag (gtag.js) -->
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-GZHXTPTRRE"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-GZHXTPTRRE');
</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Winter 2025</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="https://middlebury.instructure.com/courses/16004"> 
<span class="menu-text">Canvas</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://www.gradescope.com/courses/934368"> 
<span class="menu-text">Gradescope</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../syllabus.html"> 
<span class="menu-text">Syllabus</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-prior-years" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Prior Years</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-prior-years">    
        <li>
    <a class="dropdown-item" href="https://www.rtealwitter.com/deeplearning2023/">
 <span class="dropdown-text">Winter 2023</span></a>
  </li>  
    </ul>
  </li>
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#math-review" id="toc-math-review" class="nav-link active" data-scroll-target="#math-review">Math Review</a>
  <ul class="collapse">
  <li><a href="#derivatives" id="toc-derivatives" class="nav-link" data-scroll-target="#derivatives">Derivatives</a></li>
  <li><a href="#chain-rule-and-product-rule" id="toc-chain-rule-and-product-rule" class="nav-link" data-scroll-target="#chain-rule-and-product-rule">Chain Rule and Product Rule</a></li>
  <li><a href="#gradients" id="toc-gradients" class="nav-link" data-scroll-target="#gradients">Gradients</a></li>
  <li><a href="#vector-and-matrix-multiplication" id="toc-vector-and-matrix-multiplication" class="nav-link" data-scroll-target="#vector-and-matrix-multiplication">Vector and Matrix Multiplication</a></li>
  <li><a href="#inverse-matrices" id="toc-inverse-matrices" class="nav-link" data-scroll-target="#inverse-matrices">Inverse Matrices</a></li>
  </ul></li>
  <li><a href="#univariate-linear-regression" id="toc-univariate-linear-regression" class="nav-link" data-scroll-target="#univariate-linear-regression">Univariate Linear Regression</a>
  <ul class="collapse">
  <li><a href="#goal" id="toc-goal" class="nav-link" data-scroll-target="#goal">Goal</a></li>
  <li><a href="#linear-model" id="toc-linear-model" class="nav-link" data-scroll-target="#linear-model">Linear Model</a></li>
  <li><a href="#mean-squared-error-loss" id="toc-mean-squared-error-loss" class="nav-link" data-scroll-target="#mean-squared-error-loss">Mean Squared Error Loss</a></li>
  <li><a href="#exact-optimization" id="toc-exact-optimization" class="nav-link" data-scroll-target="#exact-optimization">Exact Optimization</a></li>
  </ul></li>
  <li><a href="#multivariate-linear-regression" id="toc-multivariate-linear-regression" class="nav-link" data-scroll-target="#multivariate-linear-regression">Multivariate Linear Regression</a>
  <ul class="collapse">
  <li><a href="#linear-model-1" id="toc-linear-model-1" class="nav-link" data-scroll-target="#linear-model-1">Linear Model</a></li>
  <li><a href="#mean-squared-error" id="toc-mean-squared-error" class="nav-link" data-scroll-target="#mean-squared-error">Mean Squared Error</a></li>
  <li><a href="#exact-optimization-1" id="toc-exact-optimization-1" class="nav-link" data-scroll-target="#exact-optimization-1">Exact Optimization</a></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-page-left" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><strong>Linear Regression and Mean Squared Error</strong></h1>
</div>



<div class="quarto-title-meta column-page-left">

    
  
    
  </div>
  


</header>


<section id="math-review" class="level2">
<h2 class="anchored" data-anchor-id="math-review">Math Review</h2>
<p>When I first heard about machine <em>learning</em>, I imagined a computer that was rewarded every time it gave the right answer. Maybe there were electric carrots and sticks that no one had bothered to tell me about? While I now know as little as I did then about computer hardware, I have learned that machine learning is fundamentally a mathematical process.</p>
<p>Luckily, we’ve been learning about the very mathematical ideas that make machine learning work for years! We’ll review the basics of these concepts and then jump in to linear regression, arguably the foundation of neural networks.</p>
<section id="derivatives" class="level3">
<h3 class="anchored" data-anchor-id="derivatives">Derivatives</h3>
<p>Imagine a function <span class="math inline">\(\mathcal{L}: \mathbb{R} \to \mathbb{R}\)</span>. (Instead of the usual <span class="math inline">\(f\)</span>, we’ll use <span class="math inline">\(\mathcal{L}\)</span> for reasons that will soon become clear.) The mapping notation means that <span class="math inline">\(\mathcal{L}\)</span> takes a single real number as input and outputs a single real number. In general, mathematicians tell us to be careful about whether we can differentiate a function. But, we’re computer scientists so we’ll risk it for the biscuit.</p>
<p>Let <span class="math inline">\(z \in \mathbb{R}\)</span> be the input to <span class="math inline">\(\mathcal{L}\)</span>. The derivative of <span class="math inline">\(\mathcal{L}\)</span> with respect to its input <span class="math inline">\(z\)</span> is mathematically denoted by <span class="math inline">\(\frac{\partial}{\partial z}[\mathcal{L}(z)]\)</span>.</p>
<p>Formally, the derivative is defined as <span class="math display">\[
\frac{\partial}{\partial z}[\mathcal{L}(z)]
= \lim_{h \to 0} \frac{\mathcal{L}(z + h) - \mathcal{L}(z)}{h}.
\]</span> If we were to plot <span class="math inline">\(\mathcal{L}\)</span>, the derivative at a point <span class="math inline">\(z\)</span> would be the slope of the tangent line to the curve at that point.</p>
<p>Here are several examples of functions and their derivatives that you might remember from calculus.</p>
<table style="width: 100%; border-collapse: collapse; text-align: center;">
<tbody><tr>
<td>
<b>Function: <span class="math inline">\(\mathcal{L}(z)\)</span> </b>
</td>
<td>
<b>Derivative: <span class="math inline">\(\frac{\partial}{\partial z}[\mathcal{L}(z)]\)</span></b>
</td>
</tr>
<tr>
<td>
<span class="math display">\[z^2\]</span>
</td>
<td>
<span class="math display">\[2z\]</span>
</td>
</tr>
<tr>
<td>
<span class="math display">\[z^a\]</span>
</td>
<td>
<span class="math display">\[a z^{a-1}\]</span>
</td>
</tr>
<tr>
<td>
<span class="math display">\[az + b\]</span>
</td>
<td>
<span class="math display">\[a\]</span>
</td>
</tr>
<tr>
<td>
<span class="math display">\[\ln(z)\]</span>
</td>
<td>
<span class="math display">\[\frac{1}{z}\]</span>
</td>
</tr>
<tr>
<td>
<span class="math display">\[e^z\]</span>
</td>
<td>
<span class="math display">\[e^z\]</span>
</td>
</tr>
</tbody></table>
</section>
<section id="chain-rule-and-product-rule" class="level3">
<h3 class="anchored" data-anchor-id="chain-rule-and-product-rule">Chain Rule and Product Rule</h3>
<p>While working with a simple basic function is easy, we’re not always so lucky. Modern machine learning chains many many complicated functions together. Fortunately, we will think of these operations modularly.</p>
<p>Let <span class="math inline">\(g: \mathbb{R} \to \mathbb{R}\)</span> be another function. Consider the composite function <span class="math inline">\(g(\mathcal{L}(z))\)</span>.</p>
<p>By the chain rule, the derivative of <span class="math inline">\(g(\mathcal{L}(z))\)</span> with respect to <span class="math inline">\(z\)</span> is <span class="math display">\[
\frac{\partial }{\partial z}[g(\mathcal{L}(z))]
= \frac{\partial g}{\partial z}(\mathcal{L}(z))
\frac{\partial}{\partial z}[\mathcal{L}(z)].
\]</span></p>
<p>Often, we will also multiply functions together. The product rule tells us that <span class="math display">\[
\frac{\partial }{\partial z}[g(z) \mathcal{L}(z)]
= g(z) \frac{\partial}{\partial z}[\mathcal{L}(z)]
+ \mathcal{L}(z) \frac{\partial}{\partial z}[g(z)].
\]</span></p>
</section>
<section id="gradients" class="level3">
<h3 class="anchored" data-anchor-id="gradients">Gradients</h3>
<p>In machine learning, we process high-dimensional data so we are interested in functions with multivariate input. Consider <span class="math inline">\(\mathcal{L}: \mathbb{R}^d \to \mathbb{R}\)</span>. The output of the function is still a real number but the input consists of <span class="math inline">\(d\)</span> real numbers. We will use the vector <span class="math inline">\(\mathbf{z} \in \mathbb{R}^d\)</span> to represent all <span class="math inline">\(d\)</span> inputs <span class="math inline">\(z_1, \ldots, z_d\)</span>.</p>
<p>Instead of the derivative, we will talk use the partial derivative. The partial derivative with respect to <span class="math inline">\(z_i\)</span> is denoted by <span class="math inline">\(\frac{\partial}{\partial z_i}[\mathcal{L}(\mathbf{z})]\)</span>. In effect, the partial derivative tells us how <span class="math inline">\(\mathcal{L}\)</span> changes when we change <span class="math inline">\(z_i\)</span> while keeping all other inputs fixed.</p>
<p>The gradient stores all the partial derivatives in a vector. The <span class="math inline">\(i\)</span>th entry of this vector is given by the partial derivative of <span class="math inline">\(\mathcal{L}\)</span> with respect to <span class="math inline">\(z_i\)</span>. In mathematical notation, <span class="math display">\[
\nabla_\mathbf{z} \mathcal{L} = \left[\begin{smallmatrix} \frac{\partial}{\partial z_1}[\mathcal{L}(\mathbf{z})] \\ \vdots \\ \frac{\partial}{\partial z_d}[\mathcal{L}(\mathbf{z})] \\ \end{smallmatrix}\right]
\]</span></p>
<p>Just like the derivative in one dimension, the gradient contains information about the slope of <span class="math inline">\(\mathcal{L}\)</span> with respect to each of the <span class="math inline">\(d\)</span> dimensions in its input.</p>
</section>
<section id="vector-and-matrix-multiplication" class="level3">
<h3 class="anchored" data-anchor-id="vector-and-matrix-multiplication">Vector and Matrix Multiplication</h3>
<p>Vector and matrix multiplication lives at the heart of deep learning. In fact, deep learning really started to take off when researchers realized that the Graphical Processing Unit (GPU) could be used to perform gradient updates in addition to the matrix multiplication it was designed to do for gaming.</p>
<p>Consider two vectors <span class="math inline">\(\mathbf{u} \in \mathbb{R}^d\)</span> and <span class="math inline">\(\mathbf{v} \in \mathbb{R}^d\)</span>. We will use <span class="math inline">\(\mathbf{u} \cdot \mathbf{v} = \sum_{i=1}^d u_i v_i\)</span> to denote the inner product of <span class="math inline">\(\mathbf{u}\)</span> and <span class="math inline">\(\mathbf{v}\)</span>. The <span class="math inline">\(\mathcal{\ell}_2\)</span>-norm of <span class="math inline">\(v\)</span> is given by <span class="math inline">\(\|\mathbf{v}\|_2 = \sqrt{\mathbf{u} \cdot \mathbf{u}}\)</span>.</p>
<p>Consider two matrices: <span class="math inline">\(\mathbf{A} \in \mathbb{R}^{d \times m}\)</span> and <span class="math inline">\(\mathbf{B} \in \mathbb{R}^{m \times n}\)</span> where <span class="math inline">\(d \neq n\)</span>. We can only multiply two matrices when their <em>inner</em> dimension agrees. Because the number of columns in <span class="math inline">\(\mathbf{A}\)</span> is the same as the number of rows in <span class="math inline">\(\mathbf{B}\)</span>, we can compute <span class="math inline">\(\mathbf{AB}\)</span>. However, because the number of columns in <span class="math inline">\(\mathbf{B}\)</span> is not the same as the number of rows in <span class="math inline">\(\mathbf{A}\)</span>, the product <span class="math inline">\(\mathbf{BA}\)</span> is not defined.</p>
<p>When we <em>can</em> multiply two matrices, the <span class="math inline">\((i,j)\)</span> entry in <span class="math inline">\(\mathbf{AB}\)</span> is given by the inner product between the <span class="math inline">\(i\)</span>th row of <span class="math inline">\(\mathbf{A}\)</span> and the <span class="math inline">\(j\)</span>th column of <span class="math inline">\(\mathbf{B}\)</span>. The resulting dimensions of the matrix product will be the number of rows in <span class="math inline">\(\mathbf{A}\)</span> by the number of columns in <span class="math inline">\(\mathbf{B}\)</span>.</p>
</section>
<section id="inverse-matrices" class="level3">
<h3 class="anchored" data-anchor-id="inverse-matrices">Inverse Matrices</h3>
<p>If we have a scalar equation <span class="math inline">\(ax = b\)</span>, we can simply solve for <span class="math inline">\(x\)</span> by dividing both sides by <span class="math inline">\(a\)</span>. In effect, we are applying the inverse of <span class="math inline">\(a\)</span> to <span class="math inline">\(a\)</span> i.e., <span class="math inline">\(\frac1{a} a =1\)</span>. The same principle applies to matrices. The <span class="math inline">\(n \times n\)</span> identity matrix generalizes the scalar identity <span class="math inline">\(1\)</span>. This identity matrix is denoted by <span class="math inline">\(\mathbf{I}_{n \times n} \in \mathbb{R}^{n \times n}\)</span>: the on-diagonal entries <span class="math inline">\((i,i)\)</span> are 1 while the off-diagonal entries <span class="math inline">\((i,j)\)</span> for <span class="math inline">\(i\neq j\)</span> are 0.</p>
<p>Consider the matrix equation <span class="math inline">\(\mathbf{Ax} = \mathbf{b}\)</span> where <span class="math inline">\(\mathbf{A} \in \mathbb{R}^{d \times d}\)</span>, <span class="math inline">\(\mathbf{x} \in \mathbb{R}^d\)</span>, and <span class="math inline">\(\mathbf{b} \in \mathbb{R}^d\)</span>. (Notice that the inner dimensions of <span class="math inline">\(\mathbf{A}\)</span> and <span class="math inline">\(\mathbf{x}\)</span> agree so their multiplication is well-defined, and the resulting vector is the same dimension as <span class="math inline">\(\mathbf{b}\)</span>.)</p>
<p>If we want to solve for <span class="math inline">\(\mathbf{x}\)</span>, we can use the matrix inverse. For a matrix <span class="math inline">\(\mathbf{A}\)</span>, we use <span class="math inline">\(\mathbf{A}^{-1}\)</span> to denote its inverse. The inverse is defined so that <span class="math inline">\(\mathbf{A}^{-1} \mathbf{A} = \mathbf{I}_{n \times n}\)</span> where <span class="math inline">\(\mathbf{I}_{n \times n}\)</span> is the identity matrix. Then, we can solve for <span class="math inline">\(\mathbf{x}\)</span> by multiplying both sides of the equation by <span class="math inline">\(\mathbf{A}^{-1}\)</span>. <span class="math display">\[
\mathbf{A}^{-1} \mathbf{Ax} = \mathbf{A}^{-1} \mathbf{b}
\]</span> Since <span class="math inline">\(\mathbf{A}^{-1} \mathbf{A} = \mathbf{I}_{n \times n}\)</span>, we have that <span class="math inline">\(\mathbf{I}_{n \times n} \mathbf{x} = \mathbf{x} = \mathbf{A}^{-1} \mathbf{b}\)</span>.</p>
</section>
</section>
<section id="univariate-linear-regression" class="level2">
<h2 class="anchored" data-anchor-id="univariate-linear-regression">Univariate Linear Regression</h2>
<p>Now that we’ve reviewed the basic building blocks of machine learning, we can dive into linear regression. Linear regression is a simple and powerful tool that we will use to understand the basics of deep learning.</p>
<section id="goal" class="level3">
<h3 class="anchored" data-anchor-id="goal">Goal</h3>
<p>We will begin our study of deep learning in the supervised setting. In this setting, we are given labelled data with input features and an outcome. Formally, we will have <span class="math inline">\(n\)</span> labelled observations <span class="math inline">\((x^{(1)}, y^{(1)}), \ldots, (x^{(n)}, y^{(n)})\)</span>. In general, we will have <span class="math inline">\(y \in \mathbb{R}\)</span>. For simplicity, we will assume for now that <span class="math inline">\(x \in \mathbb{R}\)</span>.</p>
<p>Our goal is to process the data and learn a function that approximates the outcomes. In mathematical notation, we want to learn a function <span class="math inline">\(f: \mathbb{R} \to \mathbb{R}\)</span> so that <span class="math inline">\(f(x^{(i)}) \approx y^{(i)}\)</span> for the <span class="math inline">\(n\)</span> labelled observations <span class="math inline">\(i \in \{1,\ldots,n\}\)</span>.</p>
<p>Before we dive into the specific way we will accomplish this with linear regression, let’s discuss the general deep learning framework. This three-step framework gives a flexible scaffolding that we will use to understand almost every topic in this course.</p>
<p>The three-step framework includes:</p>
<p>• <strong>The Model:</strong> The function that we’ll use to process the input and produce a corresponding output.</p>
<p>• <strong>The Loss:</strong> The function that measures the quality of the outputs from our model. (Without loss of generality, we will assume that lower is better.)</p>
<p>• <strong>The Optimizer:</strong> The method of updating the model to improve the loss.</p>
<p>With these general concepts in mind, we’ll explore linear regression.</p>
</section>
<section id="linear-model" class="level3">
<h3 class="anchored" data-anchor-id="linear-model">Linear Model</h3>
<p>As its name suggests, linear regression uses a linear model to process the input into an approximation of the output. Let <span class="math inline">\(w \in \mathbb{R}\)</span> be a weight parameter. The linear model (for one-dimensional inputs) is given by <span class="math inline">\(f(x) = wx\)</span>.</p>
<p>Unlike many deep learning models, we can visualize the linear model since it is given by a line. In the plot, we have the <span class="math inline">\(n=10\)</span> data points plotted in 2 dimensions. There is one linear model <span class="math inline">\(f(x) = 2x\)</span> that closely approximates the data and another linear model <span class="math inline">\(f(x)=\frac12 x\)</span> that does not approximate the data.</p>
<center>
<img src="images/regression_1d.pdf" width="700">
</center>
<p>Our goal is to learn how to find a linear model that fits the data well. Before we can do this though, we will need to define what it means for a model to “fit the data well”.</p>
</section>
<section id="mean-squared-error-loss" class="level3">
<h3 class="anchored" data-anchor-id="mean-squared-error-loss">Mean Squared Error Loss</h3>
<p>Our goal for the loss function is to measure how closely the data fits the prediction made by our model. Intuitively, we should take the difference between the prediction and the true outcome <span class="math inline">\(f(x^{(i)})-y^{(i)}\)</span>.</p>
<p>The issue with this approach is that <span class="math inline">\(f(x^{(i)})-y^{(i)}\)</span> can be small (negative) even when <span class="math inline">\(f(x^{(i)}) \neq y^{(i)}\)</span>. A natural fix is to take the absolute value <span class="math inline">\(|f(x^{(i)}) - y^{(i)}|\)</span>. The benefit of the absolute value is that the loss is <span class="math inline">\(0\)</span> if and only if <span class="math inline">\(f(x^{(i)}) = y^{(i)}\)</span>. However, the absolute value function is not differentiable, which is a property we’ll need for optimization. Instead, we use the squared loss:</p>
<p><span class="math inline">\(\mathcal{L}(w) = \frac1{n} \sum_{i=1}^n (f(x^{(i)}) - y^{(i)})^2\)</span></p>
<p>Here, we use the mean squared error loss, which is the average squared difference between the prediction and the true output over the dataset. Unlike the absolute value function, the squared function is differentiable everywhere. In addition, the squared error penalizes predictions that are far from the true output even more.</p>
<center>
<img src="images/regression_losses.pdf" width="700">
</center>
<p>The plot above compares the squared function to the absolute value function. While both are <span class="math inline">\(0\)</span> if and only if their input is <span class="math inline">\(0\)</span>, the squared function is differentiable everywhere and penalizes large errors more.</p>
</section>
<section id="exact-optimization" class="level3">
<h3 class="anchored" data-anchor-id="exact-optimization">Exact Optimization</h3>
<p>We now have our model and loss function: the linear model and mean squared error loss. The question becomes how to update the weights of the model to minimize the loss. In particular, we want to find <span class="math inline">\(w\)</span> that minimizes <span class="math inline">\(\mathcal{L}(w)\)</span>. While the language we’re using is new, the problem is not. We’ve actually been studying how to do this since pre-calculus!</p>
<p>The squared loss is convex (a bowl facing up versus the downward facing <em>cave</em> of con<em>cave</em>); see the plot above for a ‘proof’ by picture. In this case, we know there is only one minimum. Not only that but we can find the minimum by setting the derivative to <span class="math inline">\(0\)</span>!</p>
<p>As such, our game plan is to set <span class="math inline">\(\frac{\partial \mathcal{L}}{\partial w}\)</span> to <span class="math inline">\(0\)</span> and solve for <span class="math inline">\(w\)</span>. Recall that <span class="math inline">\(f(x) = wx\)</span>. We will use the linearity of the derivative, the chain rule, and the power rule to compute the derivative of <span class="math inline">\(\mathcal{L}\)</span> with respect to <span class="math inline">\(w\)</span>:</p>
<p><span class="math display">\[
\frac{\partial}{\partial w}[\mathcal{L}(w)]
= \frac1{n} \sum_{i=1}^n \frac{\partial}{\partial w} [(f(x^{(i)}) - y^{(i)})^2]
= \frac1{n} \sum_{i=1}^n 2(f(x^{(i)}) - y^{(i)}) \frac{\partial}{\partial w} [(f(x^{(i)}) - y^{(i)})]
= \frac1{n} \sum_{i=1}^n 2(w x^{(i)} - y^{(i)}) x^{(i)}.
\]</span></p>
<p>Setting the derivative to <span class="math inline">\(0\)</span> and solving for <span class="math inline">\(w\)</span>, we get <span class="math inline">\(\frac2{n} \sum_{i=1}^n w (x^{(i)})^2 = \frac2{n} \sum_{i=1}^n y^{(i)} x^{(i)}\)</span> and so <span class="math display">\[
w = \frac{\sum_{i=1}^n y^{(i)}}{\sum_{i=1}^n (x^{(i)})^2}.
\]</span></p>
<p>This is the exact solution to the univariate linear regression problem! We can now use this formula to find the best linear model for our data. But we’re not done with linear regression yet. We assumed that the input was one-dimensional; however, we often have high-dimensional data.</p>
</section>
</section>
<section id="multivariate-linear-regression" class="level2">
<h2 class="anchored" data-anchor-id="multivariate-linear-regression">Multivariate Linear Regression</h2>
<p>Consider the more general setting where the input is <span class="math inline">\(d\)</span>-dimensional. As before, we observe <span class="math inline">\(n\)</span> training observations <span class="math inline">\((\mathbf{x}^{(1)}, y^{(1)}), \ldots, (\mathbf{x}^{(n)}, y^{(n)})\)</span> but now <span class="math inline">\(\mathbf{x}^{(i)} \in \mathbb{R}^d\)</span>. We will generalize the ideas from univariate linear regression to the multivariate setting.</p>
<section id="linear-model-1" class="level3">
<h3 class="anchored" data-anchor-id="linear-model-1">Linear Model</h3>
<p>Instead of using a single weight <span class="math inline">\(w \in \mathbb{R}\)</span>, we will use <span class="math inline">\(d\)</span> weights <span class="math inline">\(\mathbf{w} \in \mathbb{R}^d\)</span>. Then the model is given by <span class="math inline">\(f(x) = \mathbf{w} \cdot \mathbf{x}\)</span>.</p>
<p>Instead of using a <em>line</em> to fit the data, we use a <em>hyperplane</em>. While visualizing the model is difficult in high dimensions, we can still see the model when <span class="math inline">\(d=2\)</span>.</p>
<center>
<img src="images/regression_2d.pdf" width="600">
</center>
<p>In the plot above, we have <span class="math inline">\(n=10\)</span> data points in 3 dimensions. There is one linear model <span class="math inline">\(f(\mathbf{x}) = \begin{bmatrix} 2 \\ \frac12 \end{bmatrix} \cdot \mathbf{x}\)</span> that closely approximates the data and another linear model <span class="math inline">\(f(\mathbf{x}) = \begin{bmatrix} \frac12 \\ 0 \end{bmatrix} \cdot \mathbf{x}\)</span> that does not approximate the data.</p>
</section>
<section id="mean-squared-error" class="level3">
<h3 class="anchored" data-anchor-id="mean-squared-error">Mean Squared Error</h3>
<p>Since the output of <span class="math inline">\(f\)</span> is still a single real number, we do not have to change the loss function. However, we can use our linear algebra notation to write the mean squared error in an elegant way.</p>
<p>Let <span class="math inline">\(\mathbf{X} \in \mathbb{R}^{n \times d}\)</span> be the data matrix where the <span class="math inline">\(i\)</span>th row is <span class="math inline">\((\mathbf{x}^{(i)})^\top\)</span>. Similarly, let <span class="math inline">\(\mathbf{y} \in \mathbf{R}^n\)</span> be the target vector where the <span class="math inline">\(i\)</span>th entry is <span class="math inline">\(y^{(i)}\)</span>. We can write the mean squared error loss as <span class="math display">\[
\mathcal{L}(\mathbf{w}) = \frac1{n} \| \mathbf{X w - y} \|_2^2.
\]</span></p>
</section>
<section id="exact-optimization-1" class="level3">
<h3 class="anchored" data-anchor-id="exact-optimization-1">Exact Optimization</h3>
<p>Just like computing the derivative and setting it to <span class="math inline">\(0\)</span>, we can compute the gradient and set it to the zero vector <span class="math inline">\(\mathbf{0} \in \mathbb{R}^d\)</span>. In mathematical notation, we will set <span class="math inline">\(\nabla_\mathbf{w} \mathcal{L}(\mathbf{w}) = \mathbf{0}\)</span> and solve for <span class="math inline">\(\mathbf{w}\)</span>.</p>
<p>We will leave this as an exercise on the homework. The final solution is that <span class="math inline">\(\mathbf{w} = (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{y}\)</span>.</p>
<p>This is the exact solution to the multivariate linear regression problem!</p>
</section>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>We’ve learned about the basic building blocks of machine learning and used them to understand linear regression. We’ve seen how to define the model, loss, and optimizer. We’ve also seen how to solve the linear regression problem in both the univariate and multivariate settings.</p>
<p>There are two big questions that our discussion of linear regression raises:</p>
<ol type="1">
<li><p>What if the data does not have a linear relationship?</p></li>
<li><p>What happens when our model is too complex to solve for the exact solution?</p></li>
</ol>
<p>We will answer these questions in future lectures as we explore logistic regression, neural networks, and gradient descent.</p>



</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>