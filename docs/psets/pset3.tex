\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[a4paper, total={6in, 8in}]{geometry}
\usepackage{amsmath, amsfonts}
\usepackage{hyperref,bbm, graphicx}
\usepackage{bm}

\DeclareMathOperator*{\argmin}{arg\,min}

\title{CSCI 1051 Problem Set 3}
\author{} % TODO: Put your name here
\date{\today}

\begin{document}

\maketitle

\subsection*{Submission Instructions}

Please upload your solutions by
\textbf{5pm Friday January 24, 2025.}
\begin{itemize}
\item You are encouraged to discuss ideas
and work with your classmates. However, you
\textbf{must acknowledge} your collaborators
at the top of each solution on which
you collaborated with others 
and you \textbf{must write} your solutions
independently.
\item Your solutions to theory questions must
be written legibly, or typeset in LaTeX or markdown.
If you would like to use LaTeX, you can import the source of this document 
\href{https://www.rtealwitter.com/deeplearning/psets/pset2.tex}{here}
to Overleaf.
\item I recommend that you write your solutions to coding questions in a Jupyter notebook using Google Colab.
\item You should submit your solutions as a \textbf{single PDF} via the assignment on Gradescope.
\end{itemize}

\newpage \section*{Problem 1: Image Embeddings}

In this problem, we will embed images using an autoencoder that you train from scratch.
I recommend that use one of the datasets from the MNIST family.

\subsection*{Part A: Autoencoder Training}

Train a small (three or so layers with activations) autoencoder to produce embeddings in two dimensions via reconstruction loss.
I suggest writing one class for the encoder and one for the decoder.

\subsection*{Part B: Clustering Plot}

Take (a subset of) images in your training data, encode them with the encoder portion of the autoencoder and plot them on a scatter plot.

\subsection*{Part C: Reconstruction Plot}

Take a grid of points (about 10 by 10) in the range of points from the prior plot and pass them through your decoder.
Now plot the resulting images on a grid based on their latent dimension.

\subsection*{Extra Credit (2 points): Variational Autoencoder}

Train a variational autoencoder with reconstruction loss and variational loss simultaneously.
Remember that the encoder will reproduce two vectors that you will interpret as the mean $\bm{\mu} \in \mathbb{R}^2$ and standard deviation $\bm{\sigma} \in \mathbb{R}_+^2$ of the latent vector.
You can find the latent vector by computing $\mathbf{z} = \bm{\mu} + \bm{\sigma} \bm{\epsilon}$ where $\bm{\epsilon} \sim \mathcal{N}(\bm{0}, \bm{I}).$

The KL divergence (aka cross entropy) loss simplifies to something like
\begin{align}
    \sum_{i=1}^2 -\log(\sigma_i) + \sigma_i^2 + \mu_i^2.
\end{align}

Create the clustering and reconstruction plots for the variational autoencoder you just trained.

%\input{solutions/solution3_1}

\newpage \section*{Problem 2: Diffusion}

In this problem, we will train a diffusion model from scratch.

\subsection*{Part A: Point Distribution}

Write code to sample points from a recognizable but simple distribution in two dimensions.

\subsection*{Part B: Tuning $T$ and $\alpha$}

Recall $\mathbf{x}_t = \sqrt{\alpha^t} \mathbf{x}_0 + \sqrt{1- \alpha^t} \mathbf{z}$ for $\mathbf{z} \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$.

Write code to plot $\mathbf{x}_0, \mathbf{x}_1, \ldots, \mathbf{x}_T$. Choose the number of steps $T$ and the multiplicative weight $\alpha$ so that the images slowly turn to noise.

\subsection*{Part C: Diffusion Training}

Initialize a several layer neural network (both your input and output should be in $\mathbb{R}^2$).
Train your diffusion model to predict the corresponding $\mathbf{z}$ for a given $\mathbf{x}_t$.
Carefully tune your learning rate so the loss consistently decreases.

\subsection*{Part D: Diffusion Evaluation}

In your training loop (every say 10 iterations), sample $\mathbf{x}_T \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$ and repeatedly diffuse to $\mathbf{x}_0$ as described in class. Plot the resulting process and ensure your model learns the distribution!

%\input{solutions/solution3_2}

\newpage

\subsection*{Problem 3: Schr\"{o}dinger Bridges}

In this problem, we will train a Schroedinger Bridge from scratch.

\subsection*{Part A: Two Distributions}

Create one distribution $p_{\textnormal{data}}$ that is a recognizable but simple distribution like two balls of points.
Create another distribution $p_{\textnormal{data}}'$ that is the first distribution but shifted.

\subsection*{Part B: Diffusion Training}

Set $T \approx 20$ and $\gamma = 2/T$.
Using the simplified diffusion approach described in class, train a `forward' model $f$ to go from $\mathcal{N}(\mathbf{0}, \mathbf{I})$ to $p_{\textnormal{data}}$ and a `backward' model $b$ to go from $\mathcal{N}(\mathbf{0}, \mathbf{I})$ to $p_{\textnormal{data}}'$.

\subsection*{Part C: Transport Plot}

During training:
\begin{itemize}
    \item Sample points $x_0 \sim p_{\textnormal{data}}$ and apply the backward model $b$ for $T$ steps. Plot the progression of the points.
    \item Sample points $y_T \sim p_{\textnormal{data}}'$ and apply the forward model $f$ for $T$ steps. Plot the progression of the points.
\end{itemize}

\subsection*{Extra Credit (2 points): Schr\"{o}dinger Bridge}

Iteratively train the models together via the Schr\"{o}dinger bridge training as described in class.

During training, produce the same plots as described above. What do you notice?

\end{document}